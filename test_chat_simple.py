#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç®€å•çš„èŠå¤©æœç´¢æµ‹è¯•
"""

import asyncio
import json
from web_scraper_fixed import WebScraper

async def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    scraper = WebScraper()
    
    # æµ‹è¯•å…³é”®è¯
    query = "äººå·¥æ™ºèƒ½"
    
    print(f"ğŸ” å¼€å§‹æµ‹è¯•æœç´¢åŠŸèƒ½ï¼Œå…³é”®è¯: {query}")
    
    try:
        # æµ‹è¯•å‰3ä¸ªç½‘ç«™
        from config import AI_WEBSITES
        test_websites = AI_WEBSITES[:3]  # åªæµ‹è¯•å‰3ä¸ªç½‘ç«™
        
        if not await scraper.init_browser():
            print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
            return
        
        all_results = []
        
        for website in test_websites:
            print(f"ğŸŒ æ­£åœ¨æµ‹è¯• {website['name']}...")
            try:
                results = await scraper.search_website(website, query)
                all_results.extend(results)
                print(f"âœ… {website['name']} å®Œæˆï¼Œè·å– {len(results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"âŒ {website['name']} å¤±è´¥: {e}")
        
        await scraper.close_browser()
        
        # ä¿å­˜ç»“æœ
        if all_results:
            filename = scraper.save_results(all_results, query)
            print(f"\nğŸ“Š æµ‹è¯•å®Œæˆï¼")
            print(f"ğŸ“ æ€»å…±è·å– {len(all_results)} ä¸ªç»“æœ")
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
            print(f"\nğŸ“ å‰3ä¸ªç»“æœé¢„è§ˆ:")
            for i, result in enumerate(all_results[:3]):
                print(f"  {i+1}. {result['website']}: {result['title'][:50]}...")
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•ç»“æœ")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        await scraper.close_browser()

if __name__ == "__main__":
    asyncio.run(test_search()) 