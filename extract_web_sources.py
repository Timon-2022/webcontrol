#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å°è¯•æå–DeepSeekå›å¤ä¸­çš„ç½‘é¡µæ¥æºä¿¡æ¯
åˆ†ææ•°å­—å¼•ç”¨å’Œå¯èƒ½çš„æ¥æºé“¾æ¥
"""

import asyncio
import json
import os
import re
from datetime import datetime
from playwright.async_api import async_playwright

async def extract_web_sources():
    """å°è¯•æå–ç½‘é¡µæ¥æº"""
    print("=" * 70)
    print("ğŸ” DeepSeek ç½‘é¡µæ¥æºæå– - åˆ†æå¼•ç”¨ä¿¡æ¯")
    print("=" * 70)
    
    # ä»ä¹‹å‰çš„ç»“æœæ–‡ä»¶ä¸­åˆ†æ
    latest_file = "data/deepseek_complete_content_20250621_122531.json"
    
    if os.path.exists(latest_file):
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        content = data['paragraphs_with_keyword'][0]['content']
        print(f"ğŸ“– åˆ†æå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æŸ¥æ‰¾æ•°å­—å¼•ç”¨æ¨¡å¼
        print("\nğŸ” æŸ¥æ‰¾æ•°å­—å¼•ç”¨...")
        number_patterns = [
            r'(\d+)[ã€‚.]',  # æ•°å­—åè·Ÿå¥å·
            r'(\d+)ã€',     # æ•°å­—åè·Ÿé¡¿å·
            r'\[(\d+)\]',   # æ–¹æ‹¬å·æ•°å­—
            r'ï¼ˆ(\d+)ï¼‰',   # åœ†æ‹¬å·æ•°å­—
            r'(\d+)$',      # è¡Œæœ«æ•°å­—
        ]
        
        found_numbers = []
        for pattern in number_patterns:
            matches = re.findall(pattern, content)
            if matches:
                found_numbers.extend(matches)
                print(f"  æ¨¡å¼ '{pattern}' æ‰¾åˆ°: {matches}")
        
        # å»é‡å¹¶æ’åº
        unique_numbers = sorted(list(set(found_numbers)), key=lambda x: int(x))
        print(f"ğŸ“Š å‘ç°çš„å¼•ç”¨æ•°å­—: {unique_numbers}")
        
        # åˆ†æå†…å®¹ç»“æ„
        print("\nğŸ“ åˆ†æå†…å®¹ç»“æ„...")
        
        # æŸ¥æ‰¾å…¬å¸ä¿¡æ¯æ®µè½
        sections = {
            'å…¬å¸èƒŒæ™¯': re.search(r'1\.\s*å…¬å¸èƒŒæ™¯([^2]*)', content),
            'ä¸»è¦ä¸šåŠ¡': re.search(r'2\.\s*ä¸»è¦ä¸šåŠ¡([^3]*)', content),
            'å‘å±•å†ç¨‹': re.search(r'3\.\s*å‘å±•å†ç¨‹([^4]*)', content),
        }
        
        structured_info = {}
        for section_name, match in sections.items():
            if match:
                section_content = match.group(1).strip()
                structured_info[section_name] = section_content
                print(f"  âœ… {section_name}: {len(section_content)} å­—ç¬¦")
                
                # åœ¨æ¯ä¸ªæ®µè½ä¸­æŸ¥æ‰¾æ•°å­—å¼•ç”¨
                section_numbers = re.findall(r'(\d+)', section_content)
                if section_numbers:
                    print(f"    å¼•ç”¨æ•°å­—: {section_numbers}")
        
        # æŸ¥æ‰¾å…·ä½“çš„äº§å“å’Œåˆä½œä¿¡æ¯
        print("\nğŸ” æå–å…·ä½“ä¿¡æ¯...")
        
        # äº§å“ä¿¡æ¯
        products = re.findall(r'([A-Z][A-Za-z0-9\s]+æ‰‹æŸ„|[A-Za-z0-9]+ç³»åˆ—)', content)
        print(f"ğŸ“± å‘ç°äº§å“: {products}")
        
        # åˆä½œä¼™ä¼´
        partners = re.findall(r'(å¾®è½¯|è¿ªå£«å°¼|æ¼«å¨|ç±³å“ˆæ¸¸|Xbox|Switch)', content)
        print(f"ğŸ¤ åˆä½œä¼™ä¼´: {list(set(partners))}")
        
        # æ•°æ®æŒ‡æ ‡
        metrics = re.findall(r'(\d+(?:ä¸‡|åƒä¸‡|äº¿)?(?:ç”¨æˆ·|æ¬¾æ¸¸æˆ|å¹´))', content)
        print(f"ğŸ“Š æ•°æ®æŒ‡æ ‡: {metrics}")
        
        # åœ°ç‚¹ä¿¡æ¯
        locations = re.findall(r'(å¹¿å·|æ·±åœ³|é¦™æ¸¯|ç¾å›½|æ´›æ‰çŸ¶)', content)
        print(f"ğŸŒ æ¶‰åŠåœ°ç‚¹: {list(set(locations))}")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'source_file': latest_file,
            'analysis_type': 'web_sources_extraction',
            'found_numbers': unique_numbers,
            'structured_sections': structured_info,
            'extracted_entities': {
                'products': products,
                'partners': list(set(partners)),
                'metrics': metrics,
                'locations': list(set(locations))
            },
            'total_references': len(unique_numbers),
            'content_length': len(content)
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = f"data/deepseek_sources_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¥æºåˆ†æå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š åˆ†æç»“æœ:")
        print(f"  - å‘ç°å¼•ç”¨æ•°å­—: {len(unique_numbers)} ä¸ª")
        print(f"  - ç»“æ„åŒ–æ®µè½: {len(structured_info)} ä¸ª")
        print(f"  - äº§å“ä¿¡æ¯: {len(products)} ä¸ª")
        print(f"  - åˆä½œä¼™ä¼´: {len(set(partners))} ä¸ª")
        print(f"  - æ•°æ®æŒ‡æ ‡: {len(metrics)} ä¸ª")
        
        # æ¨æµ‹å¯èƒ½çš„ç½‘é¡µæ¥æº
        print(f"\nğŸ’¡ æ¨æµ‹çš„50ä¸ªç½‘é¡µå¯èƒ½åŒ…æ‹¬:")
        print(f"  - å®˜æ–¹ç½‘ç«™å’Œäº§å“é¡µé¢")
        print(f"  - æ–°é—»æŠ¥é“å’Œåª’ä½“æ–‡ç« ")
        print(f"  - ä¼ä¸šä¿¡æ¯æŸ¥è¯¢ç½‘ç«™")
        print(f"  - æ¸¸æˆè¡Œä¸šèµ„è®¯ç½‘ç«™")
        print(f"  - åˆä½œä¼™ä¼´å®˜æ–¹å…¬å‘Š")
        print(f"  - ç”µå•†å¹³å°äº§å“é¡µé¢")
        print(f"  - æŠ•èµ„å’Œä¼ä¸šæ•°æ®åº“")
        print(f"  - ç¤¾äº¤åª’ä½“å’Œè®ºå›è®¨è®º")
        
    else:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {latest_file}")

if __name__ == "__main__":
    asyncio.run(extract_web_sources())
