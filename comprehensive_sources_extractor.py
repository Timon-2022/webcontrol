#!/usr/bin/env python3
"""
å…¨é¢å‚è€ƒæ¥æºæå–å™¨
ç³»ç»Ÿæ€§åœ°æ»šåŠ¨å¹¶ç‚¹å‡»å³ä¾§å‚è€ƒåŒºåŸŸçš„æ‰€æœ‰æ¥æºï¼Œè·å–å®Œæ•´çš„å‚è€ƒç½‘é¡µä¿¡æ¯
"""

import asyncio
import json
import time
from datetime import datetime
from playwright.async_api import async_playwright

async def extract_page_info(page):
    """æå–é¡µé¢çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        # è·å–åŸºæœ¬ä¿¡æ¯
        url = page.url
        title = await page.title()
        
        # è·å–æè¿°
        description = ""
        try:
            desc_element = await page.query_selector('meta[name="description"]')
            if desc_element:
                description = await desc_element.get_attribute('content') or ""
        except:
            pass
        
        # è·å–å…³é”®è¯
        keywords = ""
        try:
            keywords_element = await page.query_selector('meta[name="keywords"]')
            if keywords_element:
                keywords = await keywords_element.get_attribute('keywords') or ""
        except:
            pass
        
        # è·å–H1å’ŒH2æ ‡é¢˜
        h1_texts = []
        h2_texts = []
        try:
            h1_elements = await page.query_selector_all('h1')
            for h1 in h1_elements:
                text = await h1.inner_text()
                if text.strip():
                    h1_texts.append(text.strip())
            
            h2_elements = await page.query_selector_all('h2')
            for h2 in h2_elements:
                text = await h2.inner_text()
                if text.strip():
                    h2_texts.append(text.strip())
        except:
            pass
        
        # è·å–ä¸»è¦å†…å®¹
        article_content = ""
        content_selectors = [
            'article', 'main', '.content', '.article', '.post', 
            '.entry-content', '.post-content', '.article-content',
            '[class*="content"]', '[class*="article"]'
        ]
        
        for selector in content_selectors:
            try:
                content_element = await page.query_selector(selector)
                if content_element:
                    content_text = await content_element.inner_text()
                    if len(content_text) > len(article_content):
                        article_content = content_text
            except:
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œè·å–bodyå†…å®¹
        if not article_content:
            try:
                body_element = await page.query_selector('body')
                if body_element:
                    article_content = await body_element.inner_text()
            except:
                pass
        
        # ç»Ÿè®¡é“¾æ¥å’Œå›¾ç‰‡æ•°é‡
        links_count = len(await page.query_selector_all('a'))
        images_count = len(await page.query_selector_all('img'))
        
        # è·å–åŸŸå
        domain = ""
        try:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
        except:
            pass
        
        return {
            "url": url,
            "title": title,
            "description": description,
            "keywords": keywords,
            "h1_texts": h1_texts,
            "h2_texts": h2_texts,
            "article_content": article_content[:2000],  # é™åˆ¶é•¿åº¦
            "links_count": links_count,
            "images_count": images_count,
            "domain": domain
        }
    except Exception as e:
        print(f"æå–é¡µé¢ä¿¡æ¯å‡ºé”™: {e}")
        return {}

async def scroll_and_find_sources(page):
    """æ»šåŠ¨å³ä¾§åŒºåŸŸå¹¶æŸ¥æ‰¾æ‰€æœ‰å¯ç‚¹å‡»çš„æ¥æº"""
    print("å¼€å§‹æ»šåŠ¨å³ä¾§åŒºåŸŸæŸ¥æ‰¾å‚è€ƒæ¥æº...")
    
    # æŸ¥æ‰¾å³ä¾§å‚è€ƒåŒºåŸŸ
    right_panel_selectors = [
        '[class*="search-view"]',
        '[class*="reference"]', 
        '[class*="source"]',
        '[class*="result"]'
    ]
    
    # å…ˆå°è¯•æ»šåŠ¨æ•´ä¸ªé¡µé¢ï¼Œç„¶åä¸“é—¨æ»šåŠ¨å³ä¾§åŒºåŸŸ
    for i in range(5):  # æ»šåŠ¨5æ¬¡ï¼Œç¡®ä¿åŠ è½½æ‰€æœ‰å†…å®¹
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.wait_for_timeout(2000)
        
        # å°è¯•æ»šåŠ¨å³ä¾§åŒºåŸŸ
        try:
            await page.evaluate("""
                () => {
                    // æŸ¥æ‰¾å³ä¾§åŒºåŸŸå¹¶æ»šåŠ¨
                    const rightElements = document.querySelectorAll('[class*="search-view"], [class*="reference"], [class*="source"]');
                    rightElements.forEach(el => {
                        if (el.scrollTop !== undefined) {
                            el.scrollTop = el.scrollHeight;
                        }
                    });
                }
            """)
        except:
            pass
        
        await page.wait_for_timeout(1000)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å‚è€ƒæ¥æºå…ƒç´ 
    sources = await page.evaluate("""
        () => {
            const results = [];
            const allElements = document.querySelectorAll('*');
            
            allElements.forEach((el, index) => {
                const rect = el.getBoundingClientRect();
                
                // é‡ç‚¹å…³æ³¨å³ä¾§åŒºåŸŸ (x > 700)
                if (rect.x > 700 && rect.width > 50 && rect.height > 15) {
                    const text = el.innerText || el.textContent || '';
                    
                    // åˆ¤æ–­æ˜¯å¦æ˜¯æ ‡é¢˜æˆ–é“¾æ¥
                    const isRelevant = (
                        // åŒ…å«ç›¸å…³å…³é”®è¯
                        (text.includes('å°é¸¡') || text.includes('ç›–ä¸–') || text.includes('GameSir') || 
                         text.includes('ç§‘æŠ€') || text.includes('ç½‘ç»œ') || text.includes('å…¬å¸') ||
                         text.includes('æ¸¸æˆ') || text.includes('å¤–è®¾') || text.includes('æ‰‹æŸ„')) ||
                        // æˆ–è€…æ˜¯æ ‡é¢˜æ ·å¼çš„å…ƒç´ 
                        (el.tagName === 'H1' || el.tagName === 'H2' || el.tagName === 'H3' ||
                         el.className.includes('title') || el.className.includes('heading')) ||
                        // æˆ–è€…æ˜¯å¯ç‚¹å‡»çš„å…ƒç´ 
                        (el.tagName === 'A' || el.onclick !== null || 
                         window.getComputedStyle(el).cursor === 'pointer')
                    );
                    
                    if (isRelevant && text.length > 5 && text.length < 300) {
                        results.push({
                            index: index,
                            x: rect.x,
                            y: rect.y,
                            width: rect.width,
                            height: rect.height,
                            text: text.trim(),
                            tagName: el.tagName,
                            className: el.className,
                            id: el.id,
                            href: el.href || null,
                            cursor: window.getComputedStyle(el).cursor,
                            isClickable: (
                                el.tagName === 'A' || el.onclick !== null ||
                                window.getComputedStyle(el).cursor === 'pointer' ||
                                el.querySelector('a') !== null
                            )
                        });
                    }
                }
            });
            
            // å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åº
            const unique = results.filter((item, index, self) => 
                index === self.findIndex(t => t.text === item.text)
            );
            
            return unique.sort((a, b) => {
                const scoreA = (a.isClickable ? 10 : 0) + 
                              (a.text.includes('å°é¸¡') ? 5 : 0) +
                              (a.tagName === 'A' ? 3 : 0);
                const scoreB = (b.isClickable ? 10 : 0) + 
                              (b.text.includes('å°é¸¡') ? 5 : 0) +
                              (b.tagName === 'A' ? 3 : 0);
                return scoreB - scoreA;
            });
        }
    """)
    
    print(f"æ‰¾åˆ° {len(sources)} ä¸ªæ½œåœ¨çš„å‚è€ƒæ¥æº")
    return sources

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å…¨é¢å‚è€ƒæ¥æºæå–å™¨")
    print("ç³»ç»Ÿæ€§åœ°æ»šåŠ¨å¹¶ç‚¹å‡»å³ä¾§å‚è€ƒåŒºåŸŸçš„æ‰€æœ‰æ¥æº")
    print("="*80)
    
    playwright = None
    context = None
    
    try:
        # 1. å¯åŠ¨æµè§ˆå™¨
        playwright = await async_playwright().start()
        context = await playwright.chromium.launch_persistent_context(
            user_data_dir="./deepseek_user_data",
            headless=False,
            viewport={'width': 1920, 'height': 1080}
        )
        
        if context.pages:
            page = context.pages[0]
        else:
            page = await context.new_page()
        
        # 2. è®¿é—®DeepSeek
        print("2. è®¿é—®DeepSeek...")
        await page.goto("https://chat.deepseek.com", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # 3. å‘é€æŸ¥è¯¢
        print("3. å‘é€æŸ¥è¯¢...")
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        
        # å…ˆç‚¹å‡»è”ç½‘æœç´¢
        try:
            web_search_button = await page.wait_for_selector("text=è”ç½‘æœç´¢", timeout=10000)
            await web_search_button.click()
            print("âœ… å·²ç‚¹å‡»è”ç½‘æœç´¢")
            await page.wait_for_timeout(2000)
        except:
            print("âš ï¸ æœªæ‰¾åˆ°è”ç½‘æœç´¢æŒ‰é’®ï¼Œç»§ç»­...")
        
        # è¾“å…¥æŸ¥è¯¢
        chat_input = await page.wait_for_selector("textarea", timeout=10000)
        await chat_input.fill(query)
        await chat_input.press('Enter')
        
        # 4. ç­‰å¾…å›å¤å®Œæˆ
        print("4. ç­‰å¾…å›å¤å®Œæˆ...")
        await page.wait_for_timeout(45000)  # ç­‰å¾…45ç§’ç¡®ä¿å®Œå…¨åŠ è½½
        
        # 5. æ»šåŠ¨å¹¶æŸ¥æ‰¾æ‰€æœ‰å‚è€ƒæ¥æº
        print("5. æ»šåŠ¨å¹¶æŸ¥æ‰¾æ‰€æœ‰å‚è€ƒæ¥æº...")
        sources = await scroll_and_find_sources(page)
        
        # 6. ä¿å­˜æˆªå›¾
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        await page.screenshot(path=f"comprehensive_sources_{timestamp}.png", full_page=True)
        print(f"âœ… å…¨é¡µæˆªå›¾å·²ä¿å­˜: comprehensive_sources_{timestamp}.png")
        
        # 7. é€ä¸ªç‚¹å‡»å‚è€ƒæ¥æº
        print(f"7. å¼€å§‹ç‚¹å‡» {len(sources)} ä¸ªå‚è€ƒæ¥æº...")
        
        extracted_articles = []
        successful_clicks = 0
        
        for i, source in enumerate(sources[:20]):  # é™åˆ¶å‰20ä¸ªæœ€ç›¸å…³çš„
            try:
                print(f"\nç‚¹å‡»æ¥æº {i+1}/{min(20, len(sources))}: {source['text'][:60]}...")
                
                # è®°å½•ç‚¹å‡»å‰çŠ¶æ€
                initial_pages = len(context.pages)
                current_url = page.url
                
                # å°è¯•å¤šç§ç‚¹å‡»æ–¹å¼
                click_success = False
                
                # æ–¹æ³•1: åæ ‡ç‚¹å‡»
                try:
                    await page.mouse.click(
                        source['x'] + source['width']/2, 
                        source['y'] + source['height']/2
                    )
                    await page.wait_for_timeout(3000)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é¡µé¢æˆ–URLå˜åŒ–
                    if len(context.pages) > initial_pages or page.url != current_url:
                        click_success = True
                        print("  âœ… åæ ‡ç‚¹å‡»æˆåŠŸ")
                except Exception as e1:
                    print(f"  âŒ åæ ‡ç‚¹å‡»å¤±è´¥: {e1}")
                
                # æ–¹æ³•2: JavaScriptç‚¹å‡»
                if not click_success:
                    try:
                        await page.evaluate(f"""
                            () => {{
                                const elements = document.querySelectorAll('*');
                                for (let el of elements) {{
                                    const rect = el.getBoundingClientRect();
                                    if (Math.abs(rect.x - {source['x']}) < 10 && 
                                        Math.abs(rect.y - {source['y']}) < 10) {{
                                        el.click();
                                        break;
                                    }}
                                }}
                            }}
                        """)
                        await page.wait_for_timeout(3000)
                        
                        if len(context.pages) > initial_pages or page.url != current_url:
                            click_success = True
                            print("  âœ… JavaScriptç‚¹å‡»æˆåŠŸ")
                    except Exception as e2:
                        print(f"  âŒ JavaScriptç‚¹å‡»å¤±è´¥: {e2}")
                
                # å¤„ç†ç‚¹å‡»ç»“æœ
                if click_success:
                    successful_clicks += 1
                    
                    # æ£€æŸ¥æ˜¯å¦æ‰“å¼€äº†æ–°æ ‡ç­¾é¡µ
                    if len(context.pages) > initial_pages:
                        # åˆ‡æ¢åˆ°æ–°æ ‡ç­¾é¡µ
                        new_page = context.pages[-1]
                        await new_page.wait_for_load_state('networkidle', timeout=10000)
                        
                        # æå–é¡µé¢ä¿¡æ¯
                        article_data = await extract_page_info(new_page)
                        
                        extracted_articles.append({
                            "source_index": i + 1,
                            "source_text": source['text'],
                            "click_success": True,
                            "result_type": "new_tab",
                            "article_data": article_data
                        })
                        
                        print(f"  ğŸ“„ æ–°æ ‡ç­¾é¡µ: {article_data.get('title', 'Unknown')}")
                        print(f"  ğŸ”— URL: {article_data.get('url', 'Unknown')}")
                        
                        # å…³é—­æ–°æ ‡ç­¾é¡µï¼Œå›åˆ°åŸé¡µé¢
                        await new_page.close()
                        
                    elif page.url != current_url:
                        # å½“å‰é¡µé¢URLå˜åŒ–
                        await page.wait_for_load_state('networkidle', timeout=10000)
                        
                        # æå–é¡µé¢ä¿¡æ¯
                        article_data = await extract_page_info(page)
                        
                        extracted_articles.append({
                            "source_index": i + 1,
                            "source_text": source['text'],
                            "click_success": True,
                            "result_type": "page_navigation",
                            "article_data": article_data
                        })
                        
                        print(f"  ğŸ“„ é¡µé¢è·³è½¬: {article_data.get('title', 'Unknown')}")
                        print(f"  ğŸ”— URL: {article_data.get('url', 'Unknown')}")
                        
                        # è¿”å›åŸé¡µé¢
                        await page.go_back()
                        await page.wait_for_timeout(3000)
                else:
                    extracted_articles.append({
                        "source_index": i + 1,
                        "source_text": source['text'],
                        "click_success": False,
                        "result_type": "no_response",
                        "article_data": {}
                    })
                    print("  âŒ ç‚¹å‡»æ— å“åº”")
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«æ“ä½œ
                await page.wait_for_timeout(1000)
                
            except Exception as e:
                print(f"  âŒ å¤„ç†æ¥æºæ—¶å‡ºé”™: {e}")
                extracted_articles.append({
                    "source_index": i + 1,
                    "source_text": source['text'],
                    "click_success": False,
                    "result_type": "error",
                    "error": str(e),
                    "article_data": {}
                })
        
        # 8. ä¿å­˜ç»“æœ
        result = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "total_sources_found": len(sources),
            "sources_attempted": min(20, len(sources)),
            "successful_clicks": successful_clicks,
            "extracted_articles": extracted_articles,
            "all_sources_info": sources
        }
        
        result_filename = f"comprehensive_sources_result_{timestamp}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {result_filename}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ‰¾åˆ°å‚è€ƒæ¥æº: {len(sources)} ä¸ª")
        print(f"   - å°è¯•ç‚¹å‡»: {min(20, len(sources))} ä¸ª")
        print(f"   - æˆåŠŸç‚¹å‡»: {successful_clicks} ä¸ª")
        print(f"   - æˆåŠŸç‡: {successful_clicks/min(20, len(sources))*100:.1f}%")
        
        return result
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None
        
    finally:
        if context:
            await context.close()
        if playwright:
            await playwright.stop()

if __name__ == "__main__":
    asyncio.run(main()) 