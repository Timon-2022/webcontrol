#!/usr/bin/env python3
"""
é«˜çº§å³ä¾§é¢æ¿æå–å™¨
ä¸“é—¨æŸ¥æ‰¾æœç´¢ç»“æœä¸­çš„å¯ç‚¹å‡»é“¾æ¥å’ŒåŸŸåï¼Œå¹¶å°è¯•è®¿é—®åŸå§‹ç½‘ç«™
"""

import asyncio
import json
import time
from datetime import datetime
from playwright.async_api import async_playwright
import re

async def main():
    """ä¸»å‡½æ•°"""
    print("ï¿½ï¿½ é«˜çº§å³ä¾§é¢æ¿æå–å™¨")
    print("ä¸“é—¨æŸ¥æ‰¾æœç´¢ç»“æœä¸­çš„å¯ç‚¹å‡»é“¾æ¥å’ŒåŸŸå")
    print("="*80)
    
    playwright = None
    page = None
    
    try:
        # 1. å¯åŠ¨æµè§ˆå™¨
        playwright = await async_playwright().start()
        context = await playwright.chromium.launch_persistent_context(
            user_data_dir="./deepseek_user_data",
            headless=False
        )
        
        if context.pages:
            page = context.pages[0]
        else:
            page = await context.new_page()
        
        # 2. è®¿é—®DeepSeek
        print("2. è®¿é—®DeepSeek...")
        await page.goto("https://chat.deepseek.com", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # 3. å‘é€æŸ¥è¯¢
        print("3. å‘é€æŸ¥è¯¢...")
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        chat_input = await page.wait_for_selector("textarea", timeout=10000)
        await chat_input.fill(query)
        await chat_input.press('Enter')
        
        # 4. ç­‰å¾…å›å¤å®Œæˆ
        print("4. ç­‰å¾…å›å¤å®Œæˆ...")
        await page.wait_for_timeout(40000)
        
        # 5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥
        print("5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥...")
        sources_element = None
        selectors = ["text=å·²æœç´¢åˆ°", "[class*='source']", "text=/å·²æœç´¢åˆ°\\d+ä¸ªç½‘é¡µ/"]
        
        for selector in selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    text = await element.inner_text()
                    if 'æœç´¢åˆ°' in text and ('ç½‘é¡µ' in text or 'ä¸ª' in text):
                        sources_element = element
                        print(f"âœ… æ‰¾åˆ°æºä¿¡æ¯: {text}")
                        break
                if sources_element:
                    break
            except:
                continue
        
        if sources_element:
            print("6. ç‚¹å‡»æºé“¾æ¥...")
            await sources_element.click()
            await page.wait_for_timeout(10000)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            await page.screenshot(path=f"advanced_right_panel_{timestamp}.png")
            print(f"âœ… æˆªå›¾å·²ä¿å­˜: advanced_right_panel_{timestamp}.png")
        
        # 7. æå–æ‰€æœ‰æœç´¢ç»“æœä¸­çš„åŸŸåå’Œé“¾æ¥ä¿¡æ¯
        print("7. æå–æœç´¢ç»“æœä¸­çš„åŸŸåå’Œé“¾æ¥...")
        
        # è·å–é¡µé¢ä¸Šæ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ŒæŸ¥æ‰¾åŸŸåæ¨¡å¼
        page_content = await page.evaluate("""
            () => {
                const results = [];
                const rightArea = document.querySelectorAll('*');
                
                rightArea.forEach((el, index) => {
                    const rect = el.getBoundingClientRect();
                    
                    // åªè€ƒè™‘å³ä¾§åŒºåŸŸçš„å…ƒç´  (x > 800)
                    if (rect.x > 800 && rect.width > 50 && rect.height > 20) {
                        const text = el.innerText || el.textContent || '';
                        
                        // æŸ¥æ‰¾åŸŸåæ¨¡å¼
                        const domainPattern = /([a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]?\.)+[a-zA-Z]{2,}/g;
                        const domains = text.match(domainPattern) || [];
                        
                        // æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
                        const datePattern = /\d{4}\/\d{1,2}\/\d{1,2}/g;
                        const dates = text.match(datePattern) || [];
                        
                        // æŸ¥æ‰¾æ•°å­—å¼•ç”¨æ¨¡å¼ (å¦‚ "1", "2", "3" ç­‰)
                        const numberPattern = /^\d+$/;
                        const lines = text.split('\\n');
                        const numbers = lines.filter(line => numberPattern.test(line.trim()));
                        
                        if (domains.length > 0 || dates.length > 0 || text.length > 100) {
                            results.push({
                                index: index,
                                x: rect.x,
                                y: rect.y,
                                width: rect.width,
                                height: rect.height,
                                text: text.substring(0, 300),
                                domains: domains,
                                dates: dates,
                                numbers: numbers,
                                hasClickableChild: el.querySelector('a, button, [role="button"], [onclick]') !== null
                            });
                        }
                    }
                });
                
                return results;
            }
        """)
        
        print(f"æ‰¾åˆ° {len(page_content)} ä¸ªåŒ…å«åŸŸåæˆ–é‡è¦ä¿¡æ¯çš„å…ƒç´ ")
        
        # 8. åˆ†æå’Œæ•´ç†åŸŸåä¿¡æ¯
        domain_info = {}
        all_domains = set()
        
        for item in page_content:
            for domain in item['domains']:
                all_domains.add(domain)
                if domain not in domain_info:
                    domain_info[domain] = {
                        'domain': domain,
                        'occurrences': 0,
                        'contexts': [],
                        'dates': set(),
                        'numbers': set()
                    }
                
                domain_info[domain]['occurrences'] += 1
                domain_info[domain]['contexts'].append(item['text'][:200])
                domain_info[domain]['dates'].update(item['dates'])
                domain_info[domain]['numbers'].update(item['numbers'])
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for domain in domain_info:
            domain_info[domain]['dates'] = list(domain_info[domain]['dates'])
            domain_info[domain]['numbers'] = list(domain_info[domain]['numbers'])
        
        print(f"è¯†åˆ«å‡º {len(all_domains)} ä¸ªå”¯ä¸€åŸŸå:")
        for domain in sorted(all_domains):
            print(f"  - {domain} (å‡ºç° {domain_info[domain]['occurrences']} æ¬¡)")
        
        # 9. å°è¯•è®¿é—®è¿™äº›åŸŸåå¯¹åº”çš„ç½‘ç«™
        print(f"\\n9. å°è¯•è®¿é—®å‰ {min(10, len(all_domains))} ä¸ªåŸŸå...")
        
        website_data = []
        domains_to_visit = list(sorted(all_domains, key=lambda d: domain_info[d]['occurrences'], reverse=True))[:10]
        
        for i, domain in enumerate(domains_to_visit):
            try:
                print(f"\\nè®¿é—® {i+1}/{len(domains_to_visit)}: {domain}")
                
                # æ„é€ å®Œæ•´URL
                if not domain.startswith('http'):
                    url = f"https://{domain}"
                else:
                    url = domain
                
                # åˆ›å»ºæ–°æ ‡ç­¾é¡µè®¿é—®ç½‘ç«™
                new_page = await context.new_page()
                
                try:
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                    await new_page.goto(url, timeout=15000)
                    await new_page.wait_for_load_state('load', timeout=10000)
                    
                    # æå–ç½‘ç«™ä¿¡æ¯
                    site_info = await new_page.evaluate("""
                        () => {
                            return {
                                title: document.title || '',
                                description: (document.querySelector('meta[name="description"]') || {}).content || '',
                                keywords: (document.querySelector('meta[name="keywords"]') || {}).content || '',
                                h1_texts: Array.from(document.querySelectorAll('h1')).map(h => h.innerText).slice(0, 3),
                                h2_texts: Array.from(document.querySelectorAll('h2')).map(h => h.innerText).slice(0, 5),
                                body_preview: document.body ? document.body.innerText.substring(0, 500) : '',
                                links_count: document.querySelectorAll('a[href]').length,
                                images_count: document.querySelectorAll('img').length
                            };
                        }
                    """)
                    
                    website_data.append({
                        'domain': domain,
                        'url': url,
                        'success': True,
                        'info': site_info,
                        'context_from_search': domain_info[domain]['contexts'][0][:200],
                        'search_occurrences': domain_info[domain]['occurrences'],
                        'related_dates': domain_info[domain]['dates'],
                        'related_numbers': domain_info[domain]['numbers']
                    })
                    
                    print(f"  âœ… æˆåŠŸè®¿é—®: {site_info['title'][:50]}...")
                    
                except Exception as e:
                    website_data.append({
                        'domain': domain,
                        'url': url,
                        'success': False,
                        'error': str(e),
                        'context_from_search': domain_info[domain]['contexts'][0][:200],
                        'search_occurrences': domain_info[domain]['occurrences']
                    })
                    print(f"  âŒ è®¿é—®å¤±è´¥: {str(e)[:50]}...")
                
                finally:
                    await new_page.close()
                
                # ç­‰å¾…ä¸€ä¸‹é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"  âŒ å¤„ç†åŸŸå {domain} æ—¶å‡ºé”™: {e}")
        
        # 10. ä¿å­˜ç»“æœ
        result = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'total_domains_found': len(all_domains),
            'domains_attempted': len(domains_to_visit),
            'successful_visits': len([d for d in website_data if d.get('success', False)]),
            'domain_info': domain_info,
            'website_data': website_data,
            'all_domains': list(all_domains)
        }
        
        result_filename = f"advanced_right_panel_{timestamp}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\\nâœ… ç»“æœå·²ä¿å­˜: {result_filename}")
        print(f"\\nğŸ¯ æ€»ç»“:")
        print(f"  - è¯†åˆ«åŸŸå: {result['total_domains_found']}")
        print(f"  - å°è¯•è®¿é—®: {result['domains_attempted']}")
        print(f"  - æˆåŠŸè®¿é—®: {result['successful_visits']}")
        
        if result['successful_visits'] > 0:
            print(f"\\nğŸ“„ æˆåŠŸè®¿é—®çš„ç½‘ç«™:")
            for data in website_data:
                if data.get('success', False):
                    print(f"  âœ… {data['domain']}")
                    print(f"     æ ‡é¢˜: {data['info']['title'][:60]}...")
                    print(f"     æè¿°: {data['info']['description'][:80]}...")
                    print(f"     åœ¨æœç´¢ä¸­å‡ºç°: {data['search_occurrences']} æ¬¡")
                    if data['related_dates']:
                        print(f"     ç›¸å…³æ—¥æœŸ: {', '.join(data['related_dates'][:3])}")
                    print()
        
        print(f"\\nğŸ”— æ‰€æœ‰è¯†åˆ«çš„åŸŸå:")
        for domain in sorted(all_domains):
            print(f"  - {domain}")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            if page and page.context:
                await page.context.close()
            if playwright:
                await playwright.stop()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
        except:
            pass


if __name__ == "__main__":
    asyncio.run(main())
