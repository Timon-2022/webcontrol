#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨requestsåº“æµ‹è¯•ç½‘é¡µè®¿é—®
é¿å…æµè§ˆå™¨å…¼å®¹æ€§é—®é¢˜
"""

import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup

def test_website_access():
    """æµ‹è¯•ç½‘ç«™è®¿é—®"""
    print("=" * 50)
    print("ğŸŒ ç½‘ç«™è®¿é—®æµ‹è¯• (ä½¿ç”¨requests)")
    print("=" * 50)
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    # æµ‹è¯•çš„ç½‘ç«™åˆ—è¡¨
    test_sites = [
        {
            'name': 'DeepSeek',
            'url': 'https://chat.deepseek.com',
            'description': 'DeepSeek AIèŠå¤©ç½‘ç«™'
        },
        {
            'name': 'Kimi',
            'url': 'https://kimi.moonshot.cn',
            'description': 'Kimi AIèŠå¤©ç½‘ç«™'
        },
        {
            'name': 'ç™¾åº¦',
            'url': 'https://www.baidu.com',
            'description': 'ç™¾åº¦æœç´¢ï¼ˆæµ‹è¯•ç½‘ç»œè¿æ¥ï¼‰'
        }
    ]
    
    results = []
    
    for site in test_sites:
        print(f"\nğŸ” æµ‹è¯• {site['name']} ({site['description']})")
        print(f"ğŸŒ URL: {site['url']}")
        
        try:
            # å‘é€GETè¯·æ±‚
            response = requests.get(
                site['url'], 
                headers=headers, 
                timeout=30,
                allow_redirects=True
            )
            
            print(f"âœ… çŠ¶æ€ç : {response.status_code}")
            print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–é¡µé¢æ ‡é¢˜
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            print(f"ğŸ“‹ é¡µé¢æ ‡é¢˜: {title}")
            
            # æŸ¥æ‰¾å¸¸è§çš„è¾“å…¥æ¡†å…ƒç´ 
            input_elements = []
            
            # æŸ¥æ‰¾textarea
            textareas = soup.find_all('textarea')
            if textareas:
                print(f"ğŸ“ æ‰¾åˆ° {len(textareas)} ä¸ªtextareaå…ƒç´ ")
                for i, ta in enumerate(textareas[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    placeholder = ta.get('placeholder', 'æ— placeholder')
                    input_elements.append(f"textarea[{i}]: {placeholder}")
            
            # æŸ¥æ‰¾textè¾“å…¥æ¡†
            text_inputs = soup.find_all('input', {'type': 'text'})
            if text_inputs:
                print(f"âŒ¨ï¸ æ‰¾åˆ° {len(text_inputs)} ä¸ªtextè¾“å…¥æ¡†")
                for i, inp in enumerate(text_inputs[:3]):
                    placeholder = inp.get('placeholder', 'æ— placeholder')
                    input_elements.append(f"input[{i}]: {placeholder}")
            
            # æŸ¥æ‰¾å¯èƒ½çš„èŠå¤©ç›¸å…³å…ƒç´ 
            chat_elements = soup.find_all(class_=lambda x: x and ('chat' in x.lower() or 'input' in x.lower()))
            if chat_elements:
                print(f"ğŸ’¬ æ‰¾åˆ° {len(chat_elements)} ä¸ªèŠå¤©ç›¸å…³å…ƒç´ ")
            
            # ä¿å­˜ç»“æœ
            result = {
                'website': site['name'],
                'url': site['url'],
                'status_code': response.status_code,
                'title': title,
                'content_length': len(response.text),
                'input_elements': input_elements,
                'chat_elements_count': len(chat_elements),
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
            
            results.append(result)
            
            # å¦‚æœæ˜¯DeepSeekæˆ–Kimiï¼Œä¿å­˜é¡µé¢å†…å®¹ç”¨äºåˆ†æ
            if site['name'] in ['DeepSeek', 'Kimi']:
                filename = f"data/{site['name'].lower()}_page_content.html"
                os.makedirs("data", exist_ok=True)
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"ğŸ’¾ é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            result = {
                'website': site['name'],
                'url': site['url'],
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
            results.append(result)
        
        except Exception as e:
            print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
            result = {
                'website': site['name'],
                'url': site['url'],
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
            results.append(result)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"data/web_access_test_{timestamp}.json"
    
    os.makedirs("data", exist_ok=True)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'test_type': 'web_access_test',
            'timestamp': datetime.now().isoformat(),
            'total_sites': len(test_sites),
            'successful_sites': len([r for r in results if r.get('success', False)]),
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    # æ˜¾ç¤ºæ€»ç»“
    print(f"\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    
    successful = [r for r in results if r.get('success', False)]
    failed = [r for r in results if not r.get('success', False)]
    
    print(f"âœ… æˆåŠŸè®¿é—®: {len(successful)} ä¸ªç½‘ç«™")
    print(f"âŒ è®¿é—®å¤±è´¥: {len(failed)} ä¸ªç½‘ç«™")
    
    if successful:
        print("\nâœ… æˆåŠŸçš„ç½‘ç«™:")
        for result in successful:
            print(f"  - {result['website']}: {result['title']}")
    
    if failed:
        print("\nâŒ å¤±è´¥çš„ç½‘ç«™:")
        for result in failed:
            print(f"  - {result['website']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print(f"\nğŸ’¡ è¯´æ˜:")
    print("- è¿™ç§æ–¹æ³•å¯ä»¥è®¿é—®ç½‘é¡µå†…å®¹ï¼Œä½†æ— æ³•æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’")
    print("- å¯¹äºéœ€è¦ç™»å½•çš„AIèŠå¤©ç½‘ç«™ï¼Œå¯èƒ½åªèƒ½çœ‹åˆ°ç™»å½•é¡µé¢")
    print("- å¯ä»¥åˆ†æé¡µé¢ç»“æ„ï¼Œäº†è§£ç½‘ç«™çš„åŸºæœ¬ä¿¡æ¯")
    
    return results

def analyze_deepseek_page():
    """åˆ†æDeepSeeké¡µé¢ç»“æ„"""
    print(f"\nğŸ” åˆ†æDeepSeeké¡µé¢ç»“æ„...")
    
    deepseek_file = "data/deepseek_page_content.html"
    if os.path.exists(deepseek_file):
        with open(deepseek_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        soup = BeautifulSoup(content, 'html.parser')
        
        print("ğŸ“‹ é¡µé¢å…ƒç´ åˆ†æ:")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç›¸å…³å…ƒç´ 
        all_inputs = soup.find_all(['input', 'textarea', 'button'])
        print(f"  - æ€»è¾“å…¥å…ƒç´ : {len(all_inputs)} ä¸ª")
        
        # æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„å…ƒç´ 
        keywords = ['chat', 'input', 'message', 'send', 'submit']
        for keyword in keywords:
            elements = soup.find_all(class_=lambda x: x and keyword in x.lower())
            if elements:
                print(f"  - åŒ…å«'{keyword}'çš„å…ƒç´ : {len(elements)} ä¸ª")
        
        # æŸ¥æ‰¾å¯èƒ½çš„ç™»å½•ç›¸å…³å…ƒç´ 
        login_keywords = ['login', 'signin', 'auth', 'ç™»å½•', 'æ³¨å†Œ']
        login_elements = []
        for keyword in login_keywords:
            elements = soup.find_all(string=lambda text: text and keyword in text.lower())
            login_elements.extend(elements)
        
        if login_elements:
            print(f"  - å¯èƒ½éœ€è¦ç™»å½• (æ‰¾åˆ° {len(login_elements)} ä¸ªç™»å½•ç›¸å…³æ–‡æœ¬)")
        
    else:
        print("âŒ æœªæ‰¾åˆ°DeepSeeké¡µé¢æ–‡ä»¶")

if __name__ == "__main__":
    # æ‰§è¡Œç½‘ç«™è®¿é—®æµ‹è¯•
    results = test_website_access()
    
    # åˆ†æDeepSeeké¡µé¢
    analyze_deepseek_page()
    
    print(f"\n" + "=" * 50)
    print("ğŸ æµ‹è¯•å®Œæˆ")
    print("=" * 50) 