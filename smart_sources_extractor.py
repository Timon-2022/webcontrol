#!/usr/bin/env python3
"""
æ™ºèƒ½å‚è€ƒæ¥æºæå–å™¨
ç³»ç»Ÿæ€§åœ°æ»šåŠ¨å¹¶ç‚¹å‡»å³ä¾§å‚è€ƒåŒºåŸŸçš„æ‰€æœ‰æ¥æºï¼Œè·å–å®Œæ•´çš„å‚è€ƒç½‘é¡µä¿¡æ¯
"""

import asyncio
import json
import time
from datetime import datetime
from playwright.async_api import async_playwright
from urllib.parse import urlparse

async def extract_page_summary(page):
    """æå–é¡µé¢çš„ç®€è¦ä¿¡æ¯ï¼Œç”Ÿæˆ100å­—å·¦å³çš„ç®€æŠ¥"""
    try:
        url = page.url
        title = await page.title()
        
        # è·å–æè¿°
        description = ""
        try:
            desc_element = await page.query_selector('meta[name="description"]')
            if desc_element:
                description = await desc_element.get_attribute('content') or ""
        except:
            pass
        
        # è·å–ä¸»è¦å†…å®¹
        content = ""
        content_selectors = [
            'article', 'main', '.content', '.article', '.post', 
            '.entry-content', '.post-content', '.article-content'
        ]
        
        for selector in content_selectors:
            try:
                content_element = await page.query_selector(selector)
                if content_element:
                    content_text = await content_element.inner_text()
                    if len(content_text) > len(content):
                        content = content_text
            except:
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œè·å–bodyå†…å®¹
        if not content:
            try:
                body_element = await page.query_selector('body')
                if body_element:
                    content = await body_element.inner_text()
            except:
                pass
        
        # ç”Ÿæˆç®€æŠ¥ï¼ˆ100å­—å·¦å³ï¼‰
        summary = ""
        if description:
            summary = description[:100] + "..." if len(description) > 100 else description
        elif content:
            # æå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºç®€æŠ¥
            content_clean = content.replace('\n', ' ').replace('\t', ' ').strip()
            summary = content_clean[:100] + "..." if len(content_clean) > 100 else content_clean
        
        # è·å–åŸŸå
        domain = ""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
        except:
            pass
        
        return {
            "url": url,
            "title": title,
            "domain": domain,
            "summary": summary,
            "content_length": len(content)
        }
    except Exception as e:
        print(f"æå–é¡µé¢ä¿¡æ¯å‡ºé”™: {e}")
        return {"url": page.url, "title": "", "domain": "", "summary": "", "content_length": 0}

async def scroll_and_find_sources(page):
    """æ»šåŠ¨å¹¶æŸ¥æ‰¾æ‰€æœ‰å‚è€ƒæ¥æº"""
    print("å¼€å§‹æ»šåŠ¨é¡µé¢å¹¶æŸ¥æ‰¾å‚è€ƒæ¥æº...")
    
    # å¤šæ¬¡æ»šåŠ¨ç¡®ä¿åŠ è½½æ‰€æœ‰å†…å®¹
    for i in range(6):
        print(f"  æ»šåŠ¨è½®æ¬¡ {i+1}/6")
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.wait_for_timeout(2000)
        
        # ä¸“é—¨æ»šåŠ¨å³ä¾§åŒºåŸŸ
        try:
            await page.evaluate("""
                () => {
                    const rightElements = document.querySelectorAll('*');
                    rightElements.forEach(el => {
                        const rect = el.getBoundingClientRect();
                        if (rect.x > 600 && rect.width > 300 && el.scrollHeight > el.clientHeight) {
                            el.scrollTop = el.scrollHeight;
                        }
                    });
                }
            """)
        except:
            pass
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å‚è€ƒæ¥æº
    sources = await page.evaluate("""
        () => {
            const results = [];
            const allElements = document.querySelectorAll('*');
            
            allElements.forEach((el, index) => {
                const rect = el.getBoundingClientRect();
                
                // é‡ç‚¹å…³æ³¨å³ä¾§åŒºåŸŸ
                if (rect.x > 650 && rect.width > 80 && rect.height > 10 && rect.height < 200) {
                    const text = (el.innerText || el.textContent || '').trim();
                    
                    if (text.length > 8 && text.length < 500) {
                        // è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                        let score = 0;
                        
                        // å…³é”®è¯åŒ¹é…
                        const keywords = ['å°é¸¡', 'ç›–ä¸–', 'GameSir', 'ç§‘æŠ€', 'ç½‘ç»œ', 'å…¬å¸', 'æ¸¸æˆ', 'å¤–è®¾'];
                        keywords.forEach(keyword => {
                            if (text.includes(keyword)) score += 3;
                        });
                        
                        // å…ƒç´ ç±»å‹
                        if (el.tagName === 'A') score += 5;
                        if (el.tagName.match(/^H[1-6]$/)) score += 4;
                        if (el.className.includes('title')) score += 4;
                        
                        // å¯ç‚¹å‡»æ€§
                        if (el.onclick !== null) score += 3;
                        if (window.getComputedStyle(el).cursor === 'pointer') score += 3;
                        
                        // ä½ç½®åŠ åˆ†
                        if (rect.x > 800) score += 2;
                        
                        if (score > 0) {
                            results.push({
                                index: index,
                                x: Math.round(rect.x),
                                y: Math.round(rect.y),
                                width: Math.round(rect.width),
                                height: Math.round(rect.height),
                                text: text,
                                tagName: el.tagName,
                                className: el.className,
                                score: score,
                                isClickable: (
                                    el.tagName === 'A' || 
                                    el.onclick !== null ||
                                    window.getComputedStyle(el).cursor === 'pointer'
                                )
                            });
                        }
                    }
                }
            });
            
            // å»é‡å¹¶æ’åº
            const unique = [];
            const seen = new Set();
            
            results.forEach(item => {
                const key = item.text + '_' + item.x + '_' + item.y;
                if (!seen.has(key)) {
                    seen.add(key);
                    unique.push(item);
                }
            });
            
            return unique.sort((a, b) => b.score - a.score);
        }
    """)
    
    print(f"æ‰¾åˆ° {len(sources)} ä¸ªæ½œåœ¨çš„å‚è€ƒæ¥æº")
    return sources

async def click_source(page, context, source, index):
    """å®‰å…¨åœ°ç‚¹å‡»å‚è€ƒæ¥æº"""
    try:
        print(f"\nç‚¹å‡»æ¥æº {index}: {source['text'][:50]}...")
        
        initial_pages = len(context.pages)
        initial_url = page.url
        
        # å°è¯•ç‚¹å‡»
        try:
            await page.mouse.click(
                source['x'] + source['width']/2, 
                source['y'] + source['height']/2
            )
            await page.wait_for_timeout(4000)
        except Exception as e:
            print(f"  ç‚¹å‡»å¤±è´¥: {e}")
            return {
                "index": index,
                "text": source['text'],
                "success": False,
                "error": str(e)
            }
        
        # æ£€æŸ¥ç»“æœ
        if len(context.pages) > initial_pages:
            # æ–°æ ‡ç­¾é¡µ
            new_page = context.pages[-1]
            try:
                await new_page.wait_for_load_state('networkidle', timeout=15000)
                article_data = await extract_page_summary(new_page)
                print(f"  âœ… æ–°æ ‡ç­¾é¡µ: {article_data['title'][:40]}...")
                await new_page.close()
                return {
                    "index": index,
                    "text": source['text'],
                    "success": True,
                    "type": "new_tab",
                    "data": article_data
                }
            except Exception as e:
                print(f"  å¤„ç†æ–°æ ‡ç­¾é¡µå¤±è´¥: {e}")
                await new_page.close()
        
        elif page.url != initial_url:
            # é¡µé¢è·³è½¬
            try:
                await page.wait_for_load_state('networkidle', timeout=15000)
                article_data = await extract_page_summary(page)
                print(f"  âœ… é¡µé¢è·³è½¬: {article_data['title'][:40]}...")
                await page.go_back()
                await page.wait_for_timeout(3000)
                return {
                    "index": index,
                    "text": source['text'],
                    "success": True,
                    "type": "navigation",
                    "data": article_data
                }
            except Exception as e:
                print(f"  å¤„ç†é¡µé¢è·³è½¬å¤±è´¥: {e}")
                try:
                    await page.go_back()
                    await page.wait_for_timeout(3000)
                except:
                    pass
        
        print("  âŒ ç‚¹å‡»æ— å“åº”")
        return {
            "index": index,
            "text": source['text'],
            "success": False,
            "error": "no_response"
        }
        
    except Exception as e:
        print(f"  âŒ ç‚¹å‡»è¿‡ç¨‹å‡ºé”™: {e}")
        return {
            "index": index,
            "text": source['text'],
            "success": False,
            "error": str(e)
        }

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ™ºèƒ½å‚è€ƒæ¥æºæå–å™¨")
    print("ç³»ç»Ÿæ€§æ»šåŠ¨å’Œç‚¹å‡»å³ä¾§å‚è€ƒåŒºåŸŸçš„æ‰€æœ‰æ¥æº")
    print("="*80)
    
    playwright = None
    context = None
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        playwright = await async_playwright().start()
        context = await playwright.chromium.launch_persistent_context(
            user_data_dir="./deepseek_user_data",
            headless=False,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = context.pages[0] if context.pages else await context.new_page()
        
        # è®¿é—®DeepSeek
        print("1. è®¿é—®DeepSeek...")
        await page.goto("https://chat.deepseek.com", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # å‘é€æŸ¥è¯¢
        print("2. å‘é€æŸ¥è¯¢...")
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        
        # ç‚¹å‡»è”ç½‘æœç´¢
        try:
            web_search_button = await page.wait_for_selector("text=è”ç½‘æœç´¢", timeout=10000)
            await web_search_button.click()
            print("âœ… å·²ç‚¹å‡»è”ç½‘æœç´¢")
            await page.wait_for_timeout(2000)
        except:
            print("âš ï¸ æœªæ‰¾åˆ°è”ç½‘æœç´¢æŒ‰é’®")
        
        # è¾“å…¥æŸ¥è¯¢
        chat_input = await page.wait_for_selector("textarea", timeout=10000)
        await chat_input.fill(query)
        await chat_input.press('Enter')
        
        # ç­‰å¾…å›å¤å®Œæˆ
        print("3. ç­‰å¾…å›å¤å®Œæˆ...")
        await page.wait_for_timeout(45000)
        
        # æ»šåŠ¨å¹¶æŸ¥æ‰¾å‚è€ƒæ¥æº
        print("4. æ»šåŠ¨å¹¶æŸ¥æ‰¾å‚è€ƒæ¥æº...")
        sources = await scroll_and_find_sources(page)
        
        # ä¿å­˜æˆªå›¾
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        await page.screenshot(path=f"smart_sources_{timestamp}.png", full_page=True)
        print(f"âœ… æˆªå›¾å·²ä¿å­˜: smart_sources_{timestamp}.png")
        
        # æ˜¾ç¤ºå‰10ä¸ªæœ€ç›¸å…³çš„æ¥æº
        print("\nå‰10ä¸ªæœ€ç›¸å…³çš„æ¥æº:")
        for i, source in enumerate(sources[:10]):
            print(f"{i+1}. [{source['score']}åˆ†] {source['text'][:60]}...")
        
        # ç‚¹å‡»å‚è€ƒæ¥æº
        print(f"\n5. å¼€å§‹ç‚¹å‡»å‰ {min(20, len(sources))} ä¸ªå‚è€ƒæ¥æº...")
        
        results = []
        successful = 0
        max_clicks = min(20, len(sources))
        
        for i, source in enumerate(sources[:max_clicks]):
            result = await click_source(page, context, source, i + 1)
            results.append(result)
            
            if result['success']:
                successful += 1
            
            await page.wait_for_timeout(1500)
        
        # ä¿å­˜ç»“æœ
        final_result = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "total_sources": len(sources),
            "attempted": max_clicks,
            "successful": successful,
            "success_rate": f"{successful/max_clicks*100:.1f}%",
            "results": results,
            "sources": sources
        }
        
        result_filename = f"smart_sources_result_{timestamp}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {result_filename}")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   - æ‰¾åˆ°å‚è€ƒæ¥æº: {len(sources)} ä¸ª")
        print(f"   - å°è¯•ç‚¹å‡»: {max_clicks} ä¸ª")
        print(f"   - æˆåŠŸç‚¹å‡»: {successful} ä¸ª")
        print(f"   - æˆåŠŸç‡: {successful/max_clicks*100:.1f}%")
        
        # æ˜¾ç¤ºæˆåŠŸè®¿é—®çš„é¡µé¢
        print(f"\nğŸ“„ æˆåŠŸè®¿é—®çš„é¡µé¢:")
        for result in results:
            if result['success'] and 'data' in result:
                data = result['data']
                print(f"   - {data.get('title', 'Unknown')[:50]}...")
                print(f"     URL: {data.get('url', 'Unknown')}")
                print(f"     ç®€æŠ¥: {data.get('summary', 'No summary')[:80]}...")
                print()
        
        return final_result
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None
        
    finally:
        if context:
            await context.close()
        if playwright:
            await playwright.stop()

if __name__ == "__main__":
    asyncio.run(main()) 