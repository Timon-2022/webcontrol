#!/usr/bin/env python3
"""
ç‚¹å‡»å‚è€ƒæ¥æºæ ‡é¢˜æå–å™¨
ä¸“é—¨æ¨¡æ‹Ÿäººå·¥ç‚¹å‡»å³ä¾§å‚è€ƒæ¥æºæ ‡é¢˜çš„è¡Œä¸ºï¼Œè·å–åŸæ–‡ç« çš„å®Œæ•´åœ°å€å’Œå†…å®¹
"""

import asyncio
import json
import time
from datetime import datetime
from playwright.async_api import async_playwright

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç‚¹å‡»å‚è€ƒæ¥æºæ ‡é¢˜æå–å™¨")
    print("æ¨¡æ‹Ÿäººå·¥ç‚¹å‡»å³ä¾§å‚è€ƒæ¥æºæ ‡é¢˜ï¼Œè·å–åŸæ–‡ç« åœ°å€")
    print("="*80)
    
    playwright = None
    page = None
    
    try:
        # 1. å¯åŠ¨æµè§ˆå™¨
        playwright = await async_playwright().start()
        context = await playwright.chromium.launch_persistent_context(
            user_data_dir="./deepseek_user_data",
            headless=False
        )
        
        if context.pages:
            page = context.pages[0]
        else:
            page = await context.new_page()
        
        # 2. è®¿é—®DeepSeek
        print("2. è®¿é—®DeepSeek...")
        await page.goto("https://chat.deepseek.com", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # 3. å‘é€æŸ¥è¯¢
        print("3. å‘é€æŸ¥è¯¢...")
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        chat_input = await page.wait_for_selector("textarea", timeout=10000)
        await chat_input.fill(query)
        await chat_input.press('Enter')
        
        # 4. ç­‰å¾…å›å¤å®Œæˆ
        print("4. ç­‰å¾…å›å¤å®Œæˆ...")
        await page.wait_for_timeout(40000)
        
        # 5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥
        print("5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥...")
        sources_element = None
        selectors = ["text=å·²æœç´¢åˆ°", "[class*='source']", "text=/å·²æœç´¢åˆ°\\\\d+ä¸ªç½‘é¡µ/"]
        
        for selector in selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    text = await element.inner_text()
                    if 'æœç´¢åˆ°' in text and ('ç½‘é¡µ' in text or 'ä¸ª' in text):
                        sources_element = element
                        print(f"âœ… æ‰¾åˆ°æºä¿¡æ¯: {text}")
                        break
                if sources_element:
                    break
            except:
                continue
        
        if sources_element:
            print("6. ç‚¹å‡»æºé“¾æ¥...")
            await sources_element.click()
            await page.wait_for_timeout(10000)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            await page.screenshot(path=f"click_titles_{timestamp}.png")
            print(f"âœ… æˆªå›¾å·²ä¿å­˜: click_titles_{timestamp}.png")
        
        # 7. æŸ¥æ‰¾å³ä¾§åŒºåŸŸçš„æ‰€æœ‰å¯ç‚¹å‡»æ ‡é¢˜
        print("7. æŸ¥æ‰¾å³ä¾§åŒºåŸŸçš„å¯ç‚¹å‡»æ ‡é¢˜...")
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•æŸ¥æ‰¾å¯ç‚¹å‡»çš„æ ‡é¢˜å…ƒç´ 
        clickable_titles = await page.evaluate("""
            () => {
                const results = [];
                const rightAreaElements = document.querySelectorAll('*');
                
                rightAreaElements.forEach((el, index) => {
                    const rect = el.getBoundingClientRect();
                    
                    // åªè€ƒè™‘å³ä¾§åŒºåŸŸçš„å…ƒç´  (x > 800)
                    if (rect.x > 800 && rect.width > 100 && rect.height > 20) {
                        const text = el.innerText || el.textContent || '';
                        
                        // æŸ¥æ‰¾çœ‹èµ·æ¥åƒæ ‡é¢˜çš„å…ƒç´ 
                        const isTitle = (
                            // åŒ…å«å…¬å¸åç§°ç›¸å…³å…³é”®è¯
                            (text.includes('å°é¸¡') || text.includes('ç›–ä¸–') || text.includes('GameSir') || 
                             text.includes('ç§‘æŠ€') || text.includes('ç½‘ç»œ') || text.includes('å…¬å¸')) &&
                            // æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼ˆæ ‡é¢˜é€šå¸¸ä¸ä¼šå¤ªé•¿ä¹Ÿä¸ä¼šå¤ªçŸ­ï¼‰
                            text.length > 10 && text.length < 200 &&
                            // ä¸æ˜¯çº¯æ•°å­—æˆ–æ—¥æœŸ
                            !/^\\d+$/.test(text.trim()) &&
                            !/^\\d{4}\\/\\d{1,2}\\/\\d{1,2}$/.test(text.trim())
                        );
                        
                        // æ£€æŸ¥æ˜¯å¦å¯ç‚¹å‡»
                        const isClickable = (
                            el.tagName === 'A' ||
                            el.onclick !== null ||
                            el.getAttribute('role') === 'button' ||
                            el.style.cursor === 'pointer' ||
                            window.getComputedStyle(el).cursor === 'pointer' ||
                            el.querySelector('a') !== null
                        );
                        
                        // æ£€æŸ¥æ˜¯å¦æœ‰ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
                        const hasClickListener = el.onclick !== null;
                        
                        if (isTitle || isClickable || text.length > 50) {
                            results.push({
                                index: index,
                                x: rect.x,
                                y: rect.y,
                                width: rect.width,
                                height: rect.height,
                                text: text.trim(),
                                tagName: el.tagName,
                                className: el.className,
                                id: el.id,
                                isTitle: isTitle,
                                isClickable: isClickable,
                                hasClickListener: hasClickListener,
                                hasLink: el.querySelector('a') !== null,
                                href: el.href || (el.querySelector('a') && el.querySelector('a').href) || null,
                                cursor: window.getComputedStyle(el).cursor
                            });
                        }
                    }
                });
                
                // æŒ‰ç…§æ ‡é¢˜å¯èƒ½æ€§å’Œå¯ç‚¹å‡»æ€§æ’åº
                return results.sort((a, b) => {
                    const scoreA = (a.isTitle ? 10 : 0) + (a.isClickable ? 5 : 0) + (a.hasLink ? 3 : 0);
                    const scoreB = (b.isTitle ? 10 : 0) + (b.isClickable ? 5 : 0) + (b.hasLink ? 3 : 0);
                    return scoreB - scoreA;
                });
            }
        """)
        
        print(f"æ‰¾åˆ° {len(clickable_titles)} ä¸ªå¯èƒ½çš„å¯ç‚¹å‡»æ ‡é¢˜")
        
        # 8. æ˜¾ç¤ºæ‰¾åˆ°çš„æ ‡é¢˜ä¿¡æ¯
        print("\\n8. åˆ†ææ‰¾åˆ°çš„æ ‡é¢˜:")
        for i, title in enumerate(clickable_titles[:15]):  # åªæ˜¾ç¤ºå‰15ä¸ª
            print(f"\\næ ‡é¢˜ {i+1}:")
            print(f"  æ–‡æœ¬: {title['text'][:80]}...")
            print(f"  æ ‡ç­¾: {title['tagName']}")
            print(f"  ä½ç½®: ({title['x']:.0f}, {title['y']:.0f})")
            print(f"  æ˜¯å¦æ ‡é¢˜: {'âœ…' if title['isTitle'] else 'âŒ'}")
            print(f"  æ˜¯å¦å¯ç‚¹å‡»: {'âœ…' if title['isClickable'] else 'âŒ'}")
            print(f"  åŒ…å«é“¾æ¥: {'âœ…' if title['hasLink'] else 'âŒ'}")
            print(f"  é¼ æ ‡æ ·å¼: {title['cursor']}")
            if title['href']:
                print(f"  é“¾æ¥åœ°å€: {title['href']}")
        
        # 9. ä¾æ¬¡ç‚¹å‡»æœ€æœ‰å¸Œæœ›çš„æ ‡é¢˜
        print(f"\\n9. ä¾æ¬¡ç‚¹å‡»å‰ {min(10, len(clickable_titles))} ä¸ªæœ€æœ‰å¸Œæœ›çš„æ ‡é¢˜...")
        
        extracted_articles = []
        titles_to_click = clickable_titles[:10]  # åªç‚¹å‡»å‰10ä¸ª
        
        for i, title_info in enumerate(titles_to_click):
            try:
                print(f"\\nç‚¹å‡»æ ‡é¢˜ {i+1}/{len(titles_to_click)}: {title_info['text'][:50]}...")
                
                # è®°å½•ç‚¹å‡»å‰çš„çŠ¶æ€
                before_url = page.url
                before_page_count = len(context.pages)
                
                # å°è¯•ç‚¹å‡»å…ƒç´ 
                try:
                    # æ–¹æ³•1: é€šè¿‡åæ ‡ç‚¹å‡»
                    await page.mouse.click(title_info['x'] + title_info['width']/2, 
                                         title_info['y'] + title_info['height']/2)
                    await page.wait_for_timeout(3000)
                    
                except Exception as e1:
                    print(f"  åæ ‡ç‚¹å‡»å¤±è´¥: {e1}")
                    try:
                        # æ–¹æ³•2: é€šè¿‡JavaScriptç‚¹å‡»
                        text_safe = title_info['text'][:20].replace('"', '\\"')
                        await page.evaluate(f"""
                            () => {{
                                const elements = document.querySelectorAll('*');
                                for (let el of elements) {{
                                    const rect = el.getBoundingClientRect();
                                    if (Math.abs(rect.x - {title_info['x']}) < 5 && 
                                        Math.abs(rect.y - {title_info['y']}) < 5 &&
                                        el.innerText.includes("{text_safe}")) {{
                                        el.click();
                                        break;
                                    }}
                                }}
                            }}
                        """)
                        await page.wait_for_timeout(3000)
                    except Exception as e2:
                        print(f"  JavaScriptç‚¹å‡»ä¹Ÿå¤±è´¥: {e2}")
                        continue
                
                # æ£€æŸ¥ç‚¹å‡»ç»“æœ
                after_page_count = len(context.pages)
                after_url = page.url
                
                click_result = {
                    'title_index': i + 1,
                    'title_text': title_info['text'][:200],
                    'click_success': False,
                    'result_type': '',
                    'article_data': {}
                }
                
                if after_page_count > before_page_count:
                    # æ–°çª—å£æ‰“å¼€
                    print(f"  âœ… æ‰“å¼€äº†æ–°çª—å£")
                    new_page = context.pages[-1]
                    
                    try:
                        await new_page.wait_for_load_state('load', timeout=15000)
                        
                        # æå–æ–‡ç« ä¿¡æ¯
                        article_info = await new_page.evaluate("""
                            () => {
                                return {
                                    url: window.location.href,
                                    title: document.title || '',
                                    description: (document.querySelector('meta[name="description"]') || {}).content || '',
                                    keywords: (document.querySelector('meta[name="keywords"]') || {}).content || '',
                                    author: (document.querySelector('meta[name="author"]') || {}).content || '',
                                    publish_date: (document.querySelector('meta[property="article:published_time"]') || 
                                                  document.querySelector('meta[name="publish_date"]') || {}).content || '',
                                    h1_texts: Array.from(document.querySelectorAll('h1')).map(h => h.innerText).slice(0, 3),
                                    h2_texts: Array.from(document.querySelectorAll('h2')).map(h => h.innerText).slice(0, 5),
                                    article_content: (document.querySelector('article') || 
                                                    document.querySelector('.content') || 
                                                    document.querySelector('.article-content') ||
                                                    document.querySelector('main') ||
                                                    document.body).innerText.substring(0, 2000),
                                    links_count: document.querySelectorAll('a[href]').length,
                                    images_count: document.querySelectorAll('img').length,
                                    domain: window.location.hostname
                                };
                            }
                        """)
                        
                        click_result['click_success'] = True
                        click_result['result_type'] = 'new_window'
                        click_result['article_data'] = article_info
                        
                        print(f"    ğŸ“„ æ–‡ç« æ ‡é¢˜: {article_info['title'][:60]}...")
                        print(f"    ğŸ”— æ–‡ç« URL: {article_info['url']}")
                        print(f"    ğŸ¢ åŸŸå: {article_info['domain']}")
                        
                        # ä¿å­˜å•ç‹¬çš„æ–‡ç« æˆªå›¾
                        await new_page.screenshot(path=f"article_{i+1}_{timestamp}.png")
                        print(f"    ğŸ“¸ æ–‡ç« æˆªå›¾: article_{i+1}_{timestamp}.png")
                        
                    except Exception as e:
                        print(f"    âŒ æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
                        click_result['article_data'] = {'error': str(e)}
                    
                    finally:
                        await new_page.close()
                        
                elif after_url != before_url:
                    # å½“å‰é¡µé¢è·³è½¬
                    print(f"  âœ… é¡µé¢è·³è½¬åˆ°: {after_url}")
                    
                    try:
                        await page.wait_for_load_state('load', timeout=15000)
                        
                        # æå–æ–‡ç« ä¿¡æ¯
                        article_info = await page.evaluate("""
                            () => {
                                return {
                                    url: window.location.href,
                                    title: document.title || '',
                                    description: (document.querySelector('meta[name="description"]') || {}).content || '',
                                    keywords: (document.querySelector('meta[name="keywords"]') || {}).content || '',
                                    author: (document.querySelector('meta[name="author"]') || {}).content || '',
                                    publish_date: (document.querySelector('meta[property="article:published_time"]') || 
                                                  document.querySelector('meta[name="publish_date"]') || {}).content || '',
                                    h1_texts: Array.from(document.querySelectorAll('h1')).map(h => h.innerText).slice(0, 3),
                                    h2_texts: Array.from(document.querySelectorAll('h2')).map(h => h.innerText).slice(0, 5),
                                    article_content: (document.querySelector('article') || 
                                                    document.querySelector('.content') || 
                                                    document.querySelector('.article-content') ||
                                                    document.querySelector('main') ||
                                                    document.body).innerText.substring(0, 2000),
                                    links_count: document.querySelectorAll('a[href]').length,
                                    images_count: document.querySelectorAll('img').length,
                                    domain: window.location.hostname
                                };
                            }
                        """)
                        
                        click_result['click_success'] = True
                        click_result['result_type'] = 'page_navigation'
                        click_result['article_data'] = article_info
                        
                        print(f"    ğŸ“„ æ–‡ç« æ ‡é¢˜: {article_info['title'][:60]}...")
                        print(f"    ğŸ”— æ–‡ç« URL: {article_info['url']}")
                        
                        # ä¿å­˜æ–‡ç« æˆªå›¾
                        await page.screenshot(path=f"article_{i+1}_{timestamp}.png")
                        print(f"    ğŸ“¸ æ–‡ç« æˆªå›¾: article_{i+1}_{timestamp}.png")
                        
                        # è¿”å›åŸé¡µé¢
                        await page.go_back()
                        await page.wait_for_timeout(3000)
                        
                    except Exception as e:
                        print(f"    âŒ æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
                        click_result['article_data'] = {'error': str(e)}
                        
                        # å°è¯•è¿”å›åŸé¡µé¢
                        try:
                            await page.go_back()
                            await page.wait_for_timeout(3000)
                        except:
                            pass
                else:
                    print(f"  âŒ ç‚¹å‡»åæ²¡æœ‰æ˜æ˜¾å˜åŒ–")
                
                extracted_articles.append(click_result)
                
                # æ¯æ¬¡ç‚¹å‡»åç­‰å¾…ä¸€ä¸‹
                await page.wait_for_timeout(2000)
                
            except Exception as e:
                print(f"  âŒ ç‚¹å‡»æ ‡é¢˜ {i+1} æ—¶å‡ºé”™: {e}")
                extracted_articles.append({
                    'title_index': i + 1,
                    'title_text': title_info['text'][:200],
                    'click_success': False,
                    'error': str(e)
                })
        
        # 10. ä¿å­˜ç»“æœ
        result = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'total_titles_found': len(clickable_titles),
            'titles_attempted': len(titles_to_click),
            'successful_clicks': len([a for a in extracted_articles if a.get('click_success', False)]),
            'extracted_articles': extracted_articles,
            'all_titles_info': clickable_titles
        }
        
        result_filename = f"click_titles_result_{timestamp}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\\nâœ… ç»“æœå·²ä¿å­˜: {result_filename}")
        print(f"\\nğŸ¯ æ€»ç»“:")
        print(f"  - æ‰¾åˆ°æ ‡é¢˜: {result['total_titles_found']}")
        print(f"  - å°è¯•ç‚¹å‡»: {result['titles_attempted']}")
        print(f"  - æˆåŠŸç‚¹å‡»: {result['successful_clicks']}")
        
        if result['successful_clicks'] > 0:
            print(f"\\nğŸ“„ æˆåŠŸè·å–çš„æ–‡ç« :")
            for article in extracted_articles:
                if article.get('click_success', False):
                    data = article['article_data']
                    print(f"  âœ… {article['title_text'][:50]}...")
                    print(f"     ğŸ“„ æ ‡é¢˜: {data.get('title', 'N/A')[:60]}...")
                    print(f"     ğŸ”— URL: {data.get('url', 'N/A')}")
                    print(f"     ğŸ¢ åŸŸå: {data.get('domain', 'N/A')}")
                    if data.get('publish_date'):
                        print(f"     ğŸ“… å‘å¸ƒæ—¥æœŸ: {data['publish_date']}")
                    print(f"     ğŸ“ å†…å®¹é¢„è§ˆ: {data.get('article_content', '')[:100]}...")
                    print()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            if page and page.context:
                await page.context.close()
            if playwright:
                await playwright.stop()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
        except:
            pass


if __name__ == "__main__":
    asyncio.run(main())
