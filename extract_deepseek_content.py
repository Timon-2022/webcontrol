#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æå–DeepSeekå›å¤çš„å®Œæ•´å†…å®¹
åˆ†ææœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯
"""

import asyncio
import json
import os
import re
from datetime import datetime
from playwright.async_api import async_playwright

async def extract_complete_content():
    """æå–DeepSeekçš„å®Œæ•´å›å¤å†…å®¹"""
    print("=" * 70)
    print("ğŸ“– DeepSeek å®Œæ•´å†…å®¹æå– - åˆ†ææœç´¢å›å¤")
    print("=" * 70)
    
    # åŠ è½½ç™»å½•çŠ¶æ€
    login_files = ["deepseek_login_state.json", "login_state.json"]
    login_state = None
    
    for file in login_files:
        if os.path.exists(file):
            with open(file, 'r', encoding='utf-8') as f:
                login_state = json.load(f)
            print(f"âœ… åŠ è½½ç™»å½•çŠ¶æ€: {file}")
            break
    
    if not login_state:
        print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶")
        return
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                slow_mo=500,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = await browser.new_context(storage_state=login_state)
            page = await context.new_page()
            
            print("ğŸŒ è®¿é—®DeepSeek...")
            await page.goto("https://chat.deepseek.com")
            await page.wait_for_timeout(3000)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ç‚¹å‡»è”ç½‘æœç´¢
            print("ğŸ” å¯ç”¨è”ç½‘æœç´¢...")
            web_search_btn = await page.wait_for_selector("text=è”ç½‘æœç´¢", timeout=10000)
            await web_search_btn.click()
            await page.wait_for_timeout(2000)
            
            # è¾“å…¥æœç´¢
            print("âŒ¨ï¸ è¾“å…¥æœç´¢å†…å®¹...")
            input_box = await page.wait_for_selector("textarea", timeout=10000)
            query = "å°é¸¡ç§‘æŠ€çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸»è¦ä¸šåŠ¡ã€å‘å±•å†ç¨‹ã€æœ€æ–°åŠ¨æ€"
            await input_box.fill(query)
            await page.wait_for_timeout(1000)
            await input_box.press('Enter')
            
            print("â³ ç­‰å¾…æœç´¢å’Œå›å¤å®Œæˆ...")
            await page.wait_for_timeout(15000)  # ç­‰å¾…15ç§’
            
            # è·å–æ‰€æœ‰æ¶ˆæ¯å†…å®¹
            print("ğŸ“– æå–æ‰€æœ‰æ¶ˆæ¯å†…å®¹...")
            
            # å°è¯•å¤šç§æ–¹å¼è·å–å›å¤å†…å®¹
            message_selectors = [
                ".message-content",
                ".chat-message",
                ".response",
                ".markdown",
                "[role='assistant']",
                ".assistant-message",
                ".prose"
            ]
            
            all_messages = []
            for selector in message_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    print(f"ğŸ” é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªæ¶ˆæ¯")
                    
                    for i, element in enumerate(elements):
                        text = await element.text_content()
                        if text and len(text.strip()) > 50:  # åªè¦è¾ƒé•¿çš„å†…å®¹
                            all_messages.append({
                                'selector': selector,
                                'index': i,
                                'content': text.strip(),
                                'length': len(text.strip())
                            })
                            print(f"  æ¶ˆæ¯ {i}: {len(text)} å­—ç¬¦")
                except Exception as e:
                    print(f"âŒ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
            
            # è·å–å®Œæ•´é¡µé¢æ–‡æœ¬
            print("\nğŸ“„ è·å–å®Œæ•´é¡µé¢æ–‡æœ¬...")
            full_page_text = await page.text_content('body')
            
            # åˆ†ææ–‡æœ¬ä¸­çš„ä¿¡æ¯
            print("ï¿½ï¿½ åˆ†ææ–‡æœ¬å†…å®¹...")
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"å°é¸¡ç§‘æŠ€"çš„æ®µè½
            paragraphs_with_keyword = []
            if full_page_text:
                lines = full_page_text.split('\n')
                for i, line in enumerate(lines):
                    line = line.strip()
                    if 'å°é¸¡ç§‘æŠ€' in line and len(line) > 10:
                        # è·å–ä¸Šä¸‹æ–‡
                        context_start = max(0, i-2)
                        context_end = min(len(lines), i+3)
                        context = '\n'.join(lines[context_start:context_end])
                        
                        paragraphs_with_keyword.append({
                            'line_number': i,
                            'content': line,
                            'context': context.strip(),
                            'length': len(line)
                        })
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(paragraphs_with_keyword)} ä¸ªåŒ…å«å…³é”®è¯çš„æ®µè½")
            
            # æŸ¥æ‰¾å¯èƒ½çš„ç½‘é¡µæ¥æºä¿¡æ¯
            print("ğŸ” æŸ¥æ‰¾ç½‘é¡µæ¥æºä¿¡æ¯...")
            
            # å¸¸è§çš„ç½‘é¡µæ¥æºæ¨¡å¼
            source_patterns = [
                r'æ¥æº[ï¼š:]\s*([^\n]+)',
                r'å‚è€ƒ[ï¼š:]\s*([^\n]+)',
                r'å¼•ç”¨[ï¼š:]\s*([^\n]+)',
                r'Source[ï¼š:]\s*([^\n]+)',
                r'Reference[ï¼š:]\s*([^\n]+)',
                r'æ ¹æ®[ï¼š:]?\s*([^\n]*ç½‘[^\n]*)',
                r'æ®[ï¼š:]?\s*([^\n]*ç½‘[^\n]*)',
                r'æ‘˜è‡ª[ï¼š:]\s*([^\n]+)',
                r'\[[0-9]+\]\s*([^\n]+)',  # å¼•ç”¨æ ‡è®°
                r'(https?://[^\s\)]+)',    # URL
                r'(www\.[^\s\)]+)',        # wwwé“¾æ¥
            ]
            
            found_sources = []
            if full_page_text:
                for pattern in source_patterns:
                    matches = re.findall(pattern, full_page_text, re.IGNORECASE)
                    if matches:
                        for match in matches:
                            found_sources.append({
                                'pattern': pattern,
                                'source': match.strip(),
                                'type': 'source_reference'
                            })
                        print(f"  æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(matches)} ä¸ªæ¥æº")
            
            # æŸ¥æ‰¾æ•°å­—å¼•ç”¨ï¼ˆå¦‚[1], [2]ç­‰ï¼‰
            print("ğŸ” æŸ¥æ‰¾æ•°å­—å¼•ç”¨...")
            citation_pattern = r'\[(\d+)\]'
            citations = re.findall(citation_pattern, full_page_text or '')
            print(f"ğŸ“Š æ‰¾åˆ° {len(citations)} ä¸ªæ•°å­—å¼•ç”¨: {citations}")
            
            # åˆ†æå›å¤çš„ç»“æ„
            print("ğŸ” åˆ†æå›å¤ç»“æ„...")
            
            # æŸ¥æ‰¾åˆ—è¡¨é¡¹
            list_items = []
            if full_page_text:
                # æŸ¥æ‰¾ä»¥æ•°å­—ã€å­—æ¯æˆ–ç¬¦å·å¼€å¤´çš„åˆ—è¡¨é¡¹
                list_patterns = [
                    r'^[0-9]+[.ã€]\s*(.+)$',     # æ•°å­—åˆ—è¡¨
                    r'^[â€¢Â·]\s*(.+)$',           # é¡¹ç›®ç¬¦å·
                    r'^-\s*(.+)$',              # ç ´æŠ˜å·åˆ—è¡¨
                    r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.]\s*(.+)$',  # ä¸­æ–‡æ•°å­—
                ]
                
                lines = full_page_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if 'å°é¸¡ç§‘æŠ€' in line:
                        for pattern in list_patterns:
                            match = re.match(pattern, line, re.MULTILINE)
                            if match:
                                list_items.append({
                                    'pattern': pattern,
                                    'content': match.group(1),
                                    'full_line': line
                                })
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(list_items)} ä¸ªåˆ—è¡¨é¡¹")
            
            # ä¿å­˜æ‰€æœ‰æå–çš„ä¿¡æ¯
            extracted_data = {
                'query': 'å°é¸¡ç§‘æŠ€',
                'timestamp': datetime.now().isoformat(),
                'extraction_method': 'complete_content_analysis',
                'total_messages': len(all_messages),
                'messages': all_messages,
                'paragraphs_with_keyword': paragraphs_with_keyword,
                'found_sources': found_sources,
                'citations': citations,
                'list_items': list_items,
                'full_page_length': len(full_page_text) if full_page_text else 0,
                'statistics': {
                    'total_paragraphs': len(paragraphs_with_keyword),
                    'total_sources': len(found_sources),
                    'total_citations': len(citations),
                    'total_list_items': len(list_items)
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"data/deepseek_complete_content_{timestamp}.json"
            os.makedirs("data", exist_ok=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ å®Œæ•´å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            print(f"ğŸ“Š æå–ç»Ÿè®¡:")
            print(f"  - æ¶ˆæ¯æ•°é‡: {len(all_messages)}")
            print(f"  - ç›¸å…³æ®µè½: {len(paragraphs_with_keyword)}")
            print(f"  - æ¥æºä¿¡æ¯: {len(found_sources)}")
            print(f"  - æ•°å­—å¼•ç”¨: {len(citations)}")
            print(f"  - åˆ—è¡¨é¡¹: {len(list_items)}")
            print(f"  - é¡µé¢æ€»é•¿åº¦: {len(full_page_text) if full_page_text else 0} å­—ç¬¦")
            
            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹å†…å®¹
            if paragraphs_with_keyword:
                print(f"\nğŸ“ ç›¸å…³æ®µè½ç¤ºä¾‹:")
                for i, para in enumerate(paragraphs_with_keyword[:3]):
                    print(f"  {i+1}. {para['content'][:100]}...")
            
            if found_sources:
                print(f"\nğŸ”— æ‰¾åˆ°çš„æ¥æºç¤ºä¾‹:")
                for i, source in enumerate(found_sources[:3]):
                    print(f"  {i+1}. {source['source'][:80]}...")
            
            # ä¿å­˜å®Œæ•´é¡µé¢æˆªå›¾
            await page.screenshot(path=f"deepseek_complete_{timestamp}.png", full_page=True)
            print(f"ğŸ“¸ å·²ä¿å­˜å®Œæ•´é¡µé¢æˆªå›¾")
            
            await page.wait_for_timeout(5000)
            await browser.close()
            print("âœ… å®Œæ•´å†…å®¹æå–å®Œæˆ")
            
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(extract_complete_content())
