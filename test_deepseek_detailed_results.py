#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
DeepSeekè¯¦ç»†æœç´¢ç»“æœæå–
è·å–è”ç½‘æœç´¢çš„30ä¸ªç½‘é¡µè¯¦ç»†ä¿¡æ¯
"""

import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright

async def extract_detailed_results():
    """æå–DeepSeekè”ç½‘æœç´¢çš„è¯¦ç»†ç»“æœ"""
    print("=" * 70)
    print("ğŸ” DeepSeek è¯¦ç»†æœç´¢ç»“æœæå– - è·å–30ä¸ªç½‘é¡µä¿¡æ¯")
    print("=" * 70)
    
    # åŠ è½½ç™»å½•çŠ¶æ€
    login_files = ["deepseek_login_state.json", "login_state.json"]
    login_state = None
    
    for file in login_files:
        if os.path.exists(file):
            with open(file, 'r', encoding='utf-8') as f:
                login_state = json.load(f)
            print(f"âœ… åŠ è½½ç™»å½•çŠ¶æ€: {file}")
            break
    
    if not login_state:
        print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶")
        return
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                slow_mo=800,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = await browser.new_context(storage_state=login_state)
            page = await context.new_page()
            
            print("ğŸŒ è®¿é—®DeepSeek...")
            await page.goto("https://chat.deepseek.com")
            await page.wait_for_timeout(3000)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ç‚¹å‡»è”ç½‘æœç´¢æŒ‰é’®
            print("ğŸ” ç‚¹å‡»è”ç½‘æœç´¢æŒ‰é’®...")
            web_search_btn = await page.wait_for_selector("text=è”ç½‘æœç´¢", timeout=10000)
            if web_search_btn:
                await web_search_btn.click()
                await page.wait_for_timeout(2000)
                print("âœ… å·²ç‚¹å‡»è”ç½‘æœç´¢")
            
            # è¾“å…¥æœç´¢å†…å®¹
            print("âŒ¨ï¸ è¾“å…¥æœç´¢å†…å®¹...")
            input_box = await page.wait_for_selector("textarea", timeout=10000)
            query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
            await input_box.fill(query)
            await page.wait_for_timeout(1000)
            
            print("ğŸ“¤ å‘é€æœç´¢è¯·æ±‚...")
            await input_box.press('Enter')
            
            # ç­‰å¾…æœç´¢å®Œæˆ
            print("â³ ç­‰å¾…æœç´¢å®Œæˆ...")
            await page.wait_for_timeout(10000)  # ç­‰å¾…10ç§’è®©æœç´¢å®Œæˆ
            
            # å°è¯•è·å–è¯¦ç»†çš„æœç´¢ç»“æœ
            print("ğŸ” æå–è¯¦ç»†æœç´¢ç»“æœ...")
            
            # æ–¹æ³•1: æŸ¥æ‰¾æœç´¢ç»“æœé“¾æ¥
            print("ğŸ“Š æ–¹æ³•1: æŸ¥æ‰¾æœç´¢ç»“æœé“¾æ¥...")
            search_links = []
            link_selectors = [
                "a[href*='http']",
                ".search-result a",
                ".web-result a",
                "[data-testid*='link']"
            ]
            
            for selector in link_selectors:
                try:
                    links = await page.query_selector_all(selector)
                    print(f"  é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                    
                    for i, link in enumerate(links):
                        href = await link.get_attribute('href')
                        text = await link.text_content()
                        
                        if href and href.startswith('http') and text:
                            search_links.append({
                                'url': href,
                                'title': text.strip(),
                                'selector': selector,
                                'index': i
                            })
                            print(f"    é“¾æ¥ {i}: {text.strip()[:50]}... -> {href[:50]}...")
                except Exception as e:
                    print(f"  âŒ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
            
            # æ–¹æ³•2: æŸ¥æ‰¾å¼•ç”¨/æ¥æºä¿¡æ¯
            print("\nğŸ“Š æ–¹æ³•2: æŸ¥æ‰¾å¼•ç”¨å’Œæ¥æºä¿¡æ¯...")
            source_selectors = [
                ".source",
                ".reference",
                ".citation",
                "[data-testid*='source']",
                "[data-testid*='reference']",
                ".web-source"
            ]
            
            sources = []
            for selector in source_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    print(f"  é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªæ¥æº")
                    
                    for i, element in enumerate(elements):
                        text = await element.text_content()
                        if text and len(text.strip()) > 5:
                            sources.append({
                                'text': text.strip(),
                                'selector': selector,
                                'index': i
                            })
                            print(f"    æ¥æº {i}: {text.strip()[:80]}...")
                except Exception as e:
                    print(f"  âŒ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
            
            # æ–¹æ³•3: åˆ†æå®Œæ•´çš„å›å¤å†…å®¹ï¼ŒæŸ¥æ‰¾ç½‘é¡µå¼•ç”¨
            print("\nğŸ“Š æ–¹æ³•3: åˆ†æå›å¤å†…å®¹ä¸­çš„ç½‘é¡µå¼•ç”¨...")
            page_content = await page.text_content('body')
            
            # æŸ¥æ‰¾å¯èƒ½çš„ç½‘é¡µå¼•ç”¨æ¨¡å¼
            import re
            url_patterns = [
                r'https?://[^\s\)]+',
                r'www\.[^\s\)]+',
                r'\[[0-9]+\]',  # å¼•ç”¨æ ‡è®°
                r'æ¥æºï¼š[^\n]+',
                r'å‚è€ƒï¼š[^\n]+',
                r'å¼•ç”¨ï¼š[^\n]+',
            ]
            
            web_references = []
            for pattern in url_patterns:
                matches = re.findall(pattern, page_content)
                if matches:
                    print(f"  æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
                    for match in matches[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        web_references.append({
                            'pattern': pattern,
                            'match': match,
                            'type': 'url_reference'
                        })
                        print(f"    åŒ¹é…: {match}")
            
            # æ–¹æ³•4: æŸ¥æ‰¾å…·ä½“çš„æœç´¢ç»“æœå—
            print("\nğŸ“Š æ–¹æ³•4: æŸ¥æ‰¾æœç´¢ç»“æœå—...")
            result_selectors = [
                ".search-results",
                ".web-results",
                ".result-item",
                "[data-testid*='result']",
                ".markdown ul li",
                ".markdown ol li"
            ]
            
            result_blocks = []
            for selector in result_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    print(f"  é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªç»“æœå—")
                    
                    for i, element in enumerate(elements):
                        text = await element.text_content()
                        if text and len(text.strip()) > 20:
                            result_blocks.append({
                                'text': text.strip(),
                                'selector': selector,
                                'index': i
                            })
                            print(f"    ç»“æœå— {i}: {text.strip()[:100]}...")
                except Exception as e:
                    print(f"  âŒ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results = {
                'query': 'å°é¸¡ç§‘æŠ€',
                'timestamp': datetime.now().isoformat(),
                'search_links': search_links,
                'sources': sources,
                'web_references': web_references,
                'result_blocks': result_blocks,
                'total_links': len(search_links),
                'total_sources': len(sources),
                'total_references': len(web_references),
                'total_blocks': len(result_blocks)
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"data/deepseek_detailed_results_{timestamp}.json"
            os.makedirs("data", exist_ok=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - æœç´¢é“¾æ¥: {len(search_links)} ä¸ª")
            print(f"  - æ¥æºä¿¡æ¯: {len(sources)} ä¸ª")
            print(f"  - ç½‘é¡µå¼•ç”¨: {len(web_references)} ä¸ª")
            print(f"  - ç»“æœå—: {len(result_blocks)} ä¸ª")
            
            # ä¿å­˜æœ€ç»ˆæˆªå›¾
            await page.screenshot(path=f"deepseek_detailed_{timestamp}.png", full_page=True)
            print(f"ğŸ“¸ å·²ä¿å­˜å®Œæ•´é¡µé¢æˆªå›¾: deepseek_detailed_{timestamp}.png")
            
            # å°è¯•ç‚¹å‡»å±•å¼€æ›´å¤šç»“æœ
            print("\nğŸ” å°è¯•æŸ¥æ‰¾'æŸ¥çœ‹æ›´å¤š'æˆ–'å±•å¼€'æŒ‰é’®...")
            expand_selectors = [
                "text=æŸ¥çœ‹æ›´å¤š",
                "text=å±•å¼€",
                "text=æ›´å¤šç»“æœ",
                "text=Show more",
                "button:has-text('æ›´å¤š')",
                ".expand-button",
                "[data-testid*='expand']"
            ]
            
            for selector in expand_selectors:
                try:
                    expand_btn = await page.query_selector(selector)
                    if expand_btn and await expand_btn.is_visible():
                        print(f"âœ… æ‰¾åˆ°å±•å¼€æŒ‰é’®: {selector}")
                        await expand_btn.click()
                        await page.wait_for_timeout(3000)
                        
                        # é‡æ–°æˆªå›¾
                        await page.screenshot(path=f"deepseek_expanded_{timestamp}.png", full_page=True)
                        print("ğŸ“¸ å·²ä¿å­˜å±•å¼€åæˆªå›¾")
                        break
                except:
                    pass
            
            await page.wait_for_timeout(5000)
            await browser.close()
            print("âœ… è¯¦ç»†ç»“æœæå–å®Œæˆ")
            
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(extract_detailed_results())
