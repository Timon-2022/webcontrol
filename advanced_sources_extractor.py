#!/usr/bin/env python3
"""
é«˜çº§ç‰ˆDeepSeekæºæå–å™¨
ä¸“é—¨æå–å³ä¾§å‚è€ƒé“¾æ¥åŒºåŸŸçš„å…·ä½“æ–‡ç« é¡µé¢URLï¼Œè€Œä¸æ˜¯ç½‘ç«™é¦–é¡µ
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from playwright.async_api import async_playwright, Page, Browser
import re
from urllib.parse import urlparse, urljoin

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedSourcesExtractor:
    """é«˜çº§ç‰ˆDeepSeekæºæå–å™¨"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # ç™»å½•çŠ¶æ€æ–‡ä»¶
        self.login_state_file = "login_state.json"
        
        # å¸¸è§æ–°é—»å’Œå†…å®¹ç½‘ç«™çš„URLæ¨¡å¼
        self.content_url_patterns = [
            r'.*/(article|news|post|story|content|detail|page)/.*',
            r'.*/\d{4}/\d{2}/.*',  # æ—¥æœŸæ ¼å¼URL
            r'.*/[^/]+\.html$',    # HTMLæ–‡ç« é¡µé¢
            r'.*/p/\d+',           # çŸ¥ä¹ç­‰å¹³å°çš„æ–‡ç« ID
            r'.*/articles?/\d+',   # æ–‡ç« IDæ ¼å¼
            r'.*/(tech|business|finance|company)/.*',  # åˆ†ç±»é¡µé¢
        ]
        
        # éœ€è¦è¿‡æ»¤çš„æ— å…³URLæ¨¡å¼
        self.filter_patterns = [
            r'.*\.(css|js|png|jpg|jpeg|gif|svg|ico|woff|ttf)$',
            r'.*/static/.*',
            r'.*/assets/.*',
            r'.*/cdn/.*',
            r'.*google.*fonts.*',
            r'.*widget\..*',
            r'.*analytics.*',
            r'.*tracking.*',
        ]

    async def init_browser(self) -> bool:
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            self.playwright = await async_playwright().start()
            
            # å¯åŠ¨æµè§ˆå™¨
            self.browser = await self.playwright.chromium.launch(
                headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ä¾¿äºè°ƒè¯•
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--window-size=1920,1080'
                ]
            )
            
            # åˆ›å»ºé¡µé¢
            self.page = await self.browser.new_page()
            
            # è®¾ç½®ç”¨æˆ·ä»£ç†
            await self.page.set_extra_http_headers({
                'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                             "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            })
            
            # åŠ è½½ç™»å½•çŠ¶æ€
            await self.load_login_state()
            
            logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def load_login_state(self):
        """åŠ è½½ç™»å½•çŠ¶æ€"""
        try:
            with open(self.login_state_file, 'r', encoding='utf-8') as f:
                login_data = json.load(f)
            
            if 'cookies' in login_data:
                await self.page.context.add_cookies(login_data['cookies'])
                logger.info("ç™»å½•çŠ¶æ€åŠ è½½æˆåŠŸ")
        except FileNotFoundError:
            logger.warning("æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶ï¼Œå°†ä½¿ç”¨åŒ¿åæ¨¡å¼")
        except Exception as e:
            logger.error(f"åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥: {e}")

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")

    def is_article_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ç« é¡µé¢URL"""
        if not url or len(url) < 10:
            return False
        
        # è¿‡æ»¤é™æ€èµ„æº
        for pattern in self.filter_patterns:
            if re.match(pattern, url, re.IGNORECASE):
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ–‡ç« URLæ¨¡å¼
        for pattern in self.content_url_patterns:
            if re.match(pattern, url, re.IGNORECASE):
                return True
        
        # æ£€æŸ¥URLç»“æ„ç‰¹å¾
        parsed = urlparse(url)
        path = parsed.path
        
        # è·¯å¾„åŒ…å«å¤šä¸ªå±‚çº§ä¸”ä¸æ˜¯æ ¹ç›®å½•
        if path and path != '/' and path.count('/') >= 2:
            return True
        
        # åŒ…å«æŸ¥è¯¢å‚æ•°å¯èƒ½æ˜¯æ–‡ç« é¡µé¢
        if parsed.query and any(param in parsed.query.lower() for param in ['id=', 'article=', 'post=']):
            return True
        
        return False

    async def wait_for_response_complete(self, timeout: int = 30):
        """ç­‰å¾…DeepSeekå›å¤å®Œæˆ"""
        logger.info("ç­‰å¾…DeepSeekå›å¤å®Œæˆ...")
        
        start_time = time.time()
        stable_count = 0
        last_content = ""
        
        while time.time() - start_time < timeout:
            try:
                # æŸ¥æ‰¾æœ€æ–°çš„å›å¤å…ƒç´ 
                response_elements = await self.page.query_selector_all(".ds-markdown.ds-markdown--block")
                if response_elements:
                    current_content = await response_elements[-1].inner_text()
                    
                    if current_content == last_content:
                        stable_count += 1
                        if stable_count >= 3:  # å†…å®¹ç¨³å®š3æ¬¡ï¼Œè®¤ä¸ºå›å¤å®Œæˆ
                            logger.info("å›å¤å†…å®¹å·²ç¨³å®šï¼Œè®¤ä¸ºå›å¤å®Œæˆ")
                            break
                    else:
                        stable_count = 0
                        last_content = current_content
                
                await self.page.wait_for_timeout(2000)
                
            except Exception as e:
                logger.warning(f"ç­‰å¾…å›å¤æ—¶å‡ºé”™: {e}")
                break
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
        await self.page.wait_for_timeout(3000)

    async def find_sources_info(self) -> Dict[str, Any]:
        """æŸ¥æ‰¾æºä¿¡æ¯"""
        sources_info = {
            'sources_count': 0,
            'sources_element': None,
            'sources_text': ''
        }
        
        try:
            # å¤šç§é€‰æ‹©å™¨å°è¯•æŸ¥æ‰¾æºä¿¡æ¯
            selectors = [
                "text=å·²æœç´¢åˆ°",
                "[class*='source']",
                "[class*='reference']",
                "text=/å·²æœç´¢åˆ°\\d+ä¸ªç½‘é¡µ/",
                "text=/æœç´¢åˆ°\\d+/",
                "[data-testid*='source']"
            ]
            
            for selector in selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for element in elements:
                        text = await element.inner_text()
                        if 'æœç´¢åˆ°' in text and ('ç½‘é¡µ' in text or 'ä¸ª' in text):
                            # æå–æ•°å­—
                            import re
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                sources_info['sources_count'] = int(numbers[0])
                                sources_info['sources_element'] = element
                                sources_info['sources_text'] = text
                                logger.info(f"æ‰¾åˆ°æºä¿¡æ¯: {text}")
                                return sources_info
                except:
                    continue
            
            logger.warning("æœªæ‰¾åˆ°æºä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æºä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        return sources_info

    async def scroll_and_extract_references(self) -> List[Dict[str, Any]]:
        """æ»šåŠ¨å³ä¾§å‚è€ƒé“¾æ¥åŒºåŸŸå¹¶æå–å…·ä½“æ–‡ç« URL"""
        references = []
        
        try:
            logger.info("å¼€å§‹æ»šåŠ¨å’Œæå–å³ä¾§å‚è€ƒé“¾æ¥...")
            
            # æŸ¥æ‰¾å³ä¾§å‚è€ƒé“¾æ¥åŒºåŸŸçš„å¤šç§å¯èƒ½é€‰æ‹©å™¨
            reference_area_selectors = [
                "[class*='reference']",
                "[class*='source']", 
                "[class*='citation']",
                "[class*='sidebar']",
                "[class*='panel']",
                ".ds-chat-message-content",
                "[data-testid*='reference']",
                "[data-testid*='source']"
            ]
            
            reference_area = None
            for selector in reference_area_selectors:
                try:
                    areas = await self.page.query_selector_all(selector)
                    for area in areas:
                        # æ£€æŸ¥åŒºåŸŸæ˜¯å¦åŒ…å«é“¾æ¥
                        links = await area.query_selector_all("a[href]")
                        if len(links) > 3:  # å¦‚æœåŒ…å«å¤šä¸ªé“¾æ¥ï¼Œå¯èƒ½æ˜¯å‚è€ƒåŒºåŸŸ
                            reference_area = area
                            logger.info(f"æ‰¾åˆ°å‚è€ƒé“¾æ¥åŒºåŸŸ: {selector}")
                            break
                    if reference_area:
                        break
                except:
                    continue
            
            if not reference_area:
                logger.warning("æœªæ‰¾åˆ°å‚è€ƒé“¾æ¥åŒºåŸŸï¼Œå°è¯•åœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾é“¾æ¥")
                reference_area = self.page
            
            # æ»šåŠ¨å‚è€ƒåŒºåŸŸä»¥åŠ è½½æ›´å¤šå†…å®¹
            try:
                # å¤šæ¬¡æ»šåŠ¨ä»¥ç¡®ä¿åŠ è½½æ‰€æœ‰å†…å®¹
                for i in range(5):
                    if self.page:
                        await self.page.evaluate("""
                            () => {
                                // æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                                window.scrollTo(0, document.body.scrollHeight);
                                
                                // ä¹Ÿå°è¯•æ»šåŠ¨å‚è€ƒåŒºåŸŸ
                                const referenceElements = document.querySelectorAll('[class*="reference"], [class*="source"], [class*="citation"]');
                                referenceElements.forEach(el => {
                                    if (el.scrollHeight > el.clientHeight) {
                                        el.scrollTop = el.scrollHeight;
                                    }
                                });
                            }
                        """)
                        await self.page.wait_for_timeout(1000)
                    
            except Exception as e:
                logger.warning(f"æ»šåŠ¨æ—¶å‡ºé”™: {e}")
            
            # æå–æ‰€æœ‰é“¾æ¥
            all_links = await reference_area.query_selector_all("a[href]")
            logger.info(f"æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            # åˆ†ææ¯ä¸ªé“¾æ¥
            for link in all_links:
                try:
                    href = await link.get_attribute('href')
                    text = await link.inner_text()
                    title = await link.get_attribute('title') or ''
                    
                    if href and self.is_article_url(href):
                        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                        score = self._calculate_article_relevance_score(href, text, title)
                        
                        references.append({
                            'url': href,
                            'text': text.strip()[:100],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                            'title': title.strip()[:100],
                            'score': score,
                            'domain': urlparse(href).netloc,
                            'extraction_method': 'reference_area_scroll'
                        })
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                    continue
            
            # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
            references.sort(key=lambda x: x['score'], reverse=True)
            
            # å»é‡ï¼ˆåŸºäºURLï¼‰
            seen_urls = set()
            unique_references = []
            for ref in references:
                if ref['url'] not in seen_urls:
                    seen_urls.add(ref['url'])
                    unique_references.append(ref)
            
            logger.info(f"æå–åˆ° {len(unique_references)} ä¸ªå”¯ä¸€çš„æ–‡ç« é“¾æ¥")
            return unique_references[:20]  # è¿”å›å‰20ä¸ªæœ€ç›¸å…³çš„
            
        except Exception as e:
            logger.error(f"æ»šåŠ¨å’Œæå–å‚è€ƒé“¾æ¥æ—¶å‡ºé”™: {e}")
            return []

    def _calculate_article_relevance_score(self, url: str, text: str, title: str) -> float:
        """è®¡ç®—æ–‡ç« ç›¸å…³æ€§å¾—åˆ†"""
        score = 0.0
        url_lower = url.lower()
        text_lower = text.lower()
        title_lower = title.lower()
        
        # å…³é”®è¯åŒ¹é… - å°é¸¡ç§‘æŠ€ç›¸å…³
        if 'å°é¸¡' in text_lower or 'xiaoji' in text_lower or 'å°é¸¡' in title_lower:
            score += 20.0
        if 'gamesir' in url_lower or 'gamesir' in text_lower:
            score += 15.0
        if any(keyword in text_lower or keyword in title_lower for keyword in ['ç§‘æŠ€', 'å…¬å¸', 'ä¼ä¸š']):
            score += 10.0
        if any(keyword in text_lower or keyword in title_lower for keyword in ['æ¸¸æˆ', 'æ‰‹æŸ„', 'å¤–è®¾']):
            score += 8.0
        
        # æƒå¨ç½‘ç«™åŠ åˆ†
        authoritative_domains = [
            '36kr.com', 'zhihu.com', 'baidu.com', 'sina.com.cn', 'sohu.com',
            'qq.com', 'tencent.com', 'alibaba.com', 'jd.com', 'tmall.com',
            'wikipedia.org', 'qcc.com', 'tianyancha.com'
        ]
        for domain in authoritative_domains:
            if domain in url_lower:
                score += 12.0
                break
        
        # æ–°é—»å’Œç§‘æŠ€ç½‘ç«™åŠ åˆ†
        news_keywords = ['news', 'tech', 'finance', 'business', 'company', 'startup']
        for keyword in news_keywords:
            if keyword in url_lower:
                score += 5.0
        
        # URLç»“æ„è¯„åˆ†
        if '/article/' in url_lower or '/news/' in url_lower:
            score += 8.0
        if re.search(r'/\d{4}/\d{2}/', url_lower):  # æ—¥æœŸæ ¼å¼
            score += 6.0
        if url_lower.endswith('.html'):
            score += 4.0
        
        # æ–‡æœ¬é•¿åº¦åˆç†æ€§
        if 10 <= len(text) <= 200:
            score += 3.0
        
        return score

    async def search_and_extract_advanced(self, query: str) -> Dict[str, Any]:
        """é«˜çº§æœç´¢å’Œæå–æµç¨‹"""
        result = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'success': False,
            'content': '',
            'sources_count': 0,
            'article_references': [],
            'total_references': 0,
            'error': '',
            'steps_completed': []
        }
        
        try:
            # æ­¥éª¤1: è®¿é—®DeepSeek
            logger.info("æ­¥éª¤1: è®¿é—®DeepSeek...")
            await self.page.goto("https://chat.deepseek.com", timeout=30000)
            await self.page.wait_for_timeout(3000)
            result['steps_completed'].append('è®¿é—®DeepSeek')
            
            # æ­¥éª¤2: å‘é€æŸ¥è¯¢
            logger.info("æ­¥éª¤2: å‘é€æŸ¥è¯¢...")
            chat_input = await self.page.wait_for_selector("textarea", timeout=10000)
            await chat_input.fill(query)
            await chat_input.press('Enter')
            result['steps_completed'].append('å‘é€æŸ¥è¯¢')
            
            # æ­¥éª¤3: ç­‰å¾…å›å¤å®Œæˆ
            logger.info("æ­¥éª¤3: ç­‰å¾…å›å¤å®Œæˆ...")
            await self.wait_for_response_complete()
            result['steps_completed'].append('ç­‰å¾…å›å¤å®Œæˆ')
            
            # æ­¥éª¤4: è·å–å›å¤å†…å®¹
            logger.info("æ­¥éª¤4: è·å–å›å¤å†…å®¹...")
            response_elements = await self.page.query_selector_all(".ds-markdown.ds-markdown--block")
            if response_elements:
                latest_response = response_elements[-1]
                content = await latest_response.inner_text()
                result['content'] = content[:500]  # ä¿å­˜å‰500å­—ç¬¦
                result['steps_completed'].append('è·å–å›å¤å†…å®¹')
            
            # æ­¥éª¤5: æŸ¥æ‰¾æºä¿¡æ¯
            logger.info("æ­¥éª¤5: æŸ¥æ‰¾æºä¿¡æ¯...")
            sources_info = await self.find_sources_info()
            result['sources_count'] = sources_info['sources_count']
            result['steps_completed'].append('æŸ¥æ‰¾æºä¿¡æ¯')
            
            # æ­¥éª¤6: ç‚¹å‡»æºé“¾æ¥ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
            if sources_info['sources_element']:
                logger.info("æ­¥éª¤6: ç‚¹å‡»æºé“¾æ¥...")
                try:
                    await sources_info['sources_element'].click()
                    await self.page.wait_for_timeout(3000)
                    
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    await self.page.screenshot(path=f"advanced_after_click_{timestamp}.png")
                    logger.info(f"å·²ä¿å­˜ç‚¹å‡»åæˆªå›¾: advanced_after_click_{timestamp}.png")
                    
                    result['steps_completed'].append('ç‚¹å‡»æºé“¾æ¥')
                except Exception as e:
                    logger.warning(f"ç‚¹å‡»æºé“¾æ¥å¤±è´¥: {e}")
            
            # æ­¥éª¤7: æ»šåŠ¨å¹¶æå–å‚è€ƒé“¾æ¥
            logger.info("æ­¥éª¤7: æ»šåŠ¨å¹¶æå–å‚è€ƒé“¾æ¥...")
            article_references = await self.scroll_and_extract_references()
            result['article_references'] = article_references
            result['total_references'] = len(article_references)
            result['steps_completed'].append('æ»šåŠ¨æå–å‚è€ƒé“¾æ¥')
            
            result['success'] = True
            logger.info(f"é«˜çº§æå–å®Œæˆ: æ‰¾åˆ° {result['sources_count']} ä¸ªæºï¼Œæå– {len(article_references)} ä¸ªæ–‡ç« é“¾æ¥")
            
        except Exception as e:
            logger.error(f"é«˜çº§æœç´¢å’Œæå–è¿‡ç¨‹å‡ºé”™: {e}")
            result['error'] = str(e)
        
        return result

    def save_result(self, result: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜ç»“æœ"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"advanced_sources_result_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"ç»“æœå·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return ""

    def print_result_summary(self, result: Dict[str, Any]):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print("\n" + "="*80)
        print("é«˜çº§ç‰ˆDeepSeekæºæå–ç»“æœ")
        print("="*80)
        print(f"æŸ¥è¯¢: {result['query']}")
        print(f"æ—¶é—´: {result['timestamp']}")
        print(f"æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
        print(f"å®Œæˆæ­¥éª¤: {', '.join(result['steps_completed'])}")
        
        if result['success']:
            print(f"æœç´¢ç½‘é¡µæ•°: {result['sources_count']}")
            print(f"æå–æ–‡ç« é“¾æ¥æ•°: {result['total_references']}")
            
            if result['article_references']:
                print(f"\nğŸ”— æå–åˆ°çš„æ–‡ç« é“¾æ¥ (æŒ‰ç›¸å…³æ€§æ’åº):")
                print("-" * 80)
                for i, ref in enumerate(result['article_references'][:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
                    print(f"{i}. {ref['text'][:60]}...")
                    print(f"   ğŸ“° URL: {ref['url']}")
                    print(f"   ğŸ¢ åŸŸå: {ref['domain']}")
                    print(f"   â­ ç›¸å…³æ€§å¾—åˆ†: {ref['score']:.1f}")
                    if ref['title']:
                        print(f"   ğŸ“ æ ‡é¢˜: {ref['title'][:60]}...")
                    print()
        else:
            print(f"é”™è¯¯: {result['error']}")


async def main():
    """ä¸»å‡½æ•°"""
    print("é«˜çº§ç‰ˆDeepSeekæºæå–å™¨æµ‹è¯•")
    print("="*80)
    
    extractor = AdvancedSourcesExtractor()
    
    try:
        # åˆå§‹åŒ–æµè§ˆå™¨
        if not await extractor.init_browser():
            print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
            return
        
        # æ‰§è¡Œé«˜çº§æœç´¢å’Œæå–
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        print(f"æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢: {query}")
        
        result = await extractor.search_and_extract_advanced(query)
        
        # æ˜¾ç¤ºç»“æœ
        extractor.print_result_summary(result)
        
        # ä¿å­˜ç»“æœ
        filename = extractor.save_result(result)
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    
    finally:
        # å…³é—­æµè§ˆå™¨
        await extractor.close_browser()


if __name__ == "__main__":
    asyncio.run(main())