#!/usr/bin/env python3
"""
JavaScriptæ•°æ®æŒ–æ˜å™¨
ä¸“é—¨æŸ¥æ‰¾é¡µé¢ä¸­å¯èƒ½åŒ…å«å‚è€ƒé“¾æ¥çš„JavaScriptæ•°æ®ç»“æ„
"""

import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright

async def mine_javascript_data():
    """æŒ–æ˜JavaScriptæ•°æ®"""
    print("â›ï¸ JavaScriptæ•°æ®æŒ–æ˜å™¨")
    print("æŸ¥æ‰¾é¡µé¢ä¸­çš„å‚è€ƒé“¾æ¥æ•°æ®ç»“æ„")
    print("="*80)
    
    playwright = None
    page = None
    
    try:
        # 1. å¯åŠ¨æµè§ˆå™¨
        playwright = await async_playwright().start()
        context = await playwright.chromium.launch_persistent_context(
            user_data_dir="./deepseek_user_data",
            headless=False
        )
        
        if context.pages:
            page = context.pages[0]
        else:
            page = await context.new_page()
        
        # 2. è®¿é—®DeepSeek
        print("2. è®¿é—®DeepSeek...")
        await page.goto("https://chat.deepseek.com", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # 3. å‘é€æŸ¥è¯¢
        print("3. å‘é€æŸ¥è¯¢...")
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        chat_input = await page.wait_for_selector("textarea", timeout=10000)
        await chat_input.fill(query)
        await chat_input.press('Enter')
        
        # 4. ç­‰å¾…å›å¤å®Œæˆ
        print("4. ç­‰å¾…å›å¤å®Œæˆ...")
        await page.wait_for_timeout(40000)
        
        # 5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥
        print("5. æŸ¥æ‰¾å¹¶ç‚¹å‡»æºé“¾æ¥...")
        sources_element = None
        selectors = ["text=å·²æœç´¢åˆ°", "[class*='source']", "text=/å·²æœç´¢åˆ°\\d+ä¸ªç½‘é¡µ/"]
        
        for selector in selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    text = await element.inner_text()
                    if 'æœç´¢åˆ°' in text and ('ç½‘é¡µ' in text or 'ä¸ª' in text):
                        sources_element = element
                        print(f"âœ… æ‰¾åˆ°æºä¿¡æ¯: {text}")
                        break
                if sources_element:
                    break
            except:
                continue
        
        if sources_element:
            print("6. ç‚¹å‡»æºé“¾æ¥...")
            await sources_element.click()
            await page.wait_for_timeout(10000)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            await page.screenshot(path=f"js_data_{timestamp}.png")
            print(f"âœ… æˆªå›¾å·²ä¿å­˜: js_data_{timestamp}.png")
        
        # 7. æŒ–æ˜JavaScriptæ•°æ®
        print("7. æŒ–æ˜JavaScriptæ•°æ®...")
        
        # 7.1 æŸ¥æ‰¾å…¨å±€å˜é‡ä¸­çš„URL
        print("  7.1 æŸ¥æ‰¾å…¨å±€å˜é‡ä¸­çš„URL...")
        global_urls = await page.evaluate("""
            () => {
                const urls = [];
                const urlPattern = /https?:\/\/[^\s"'<>]+/g;
                
                // æ£€æŸ¥windowå¯¹è±¡çš„æ‰€æœ‰å±æ€§
                for (let key in window) {
                    try {
                        const value = window[key];
                        if (typeof value === 'string') {
                            const matches = value.match(urlPattern);
                            if (matches) {
                                matches.forEach(url => {
                                    if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                        urls.push({
                                            source: 'window.' + key,
                                            url: url,
                                            context: value.substring(0, 200)
                                        });
                                    }
                                });
                            }
                        } else if (typeof value === 'object' && value !== null) {
                            const jsonStr = JSON.stringify(value);
                            const matches = jsonStr.match(urlPattern);
                            if (matches) {
                                matches.forEach(url => {
                                    if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                        urls.push({
                                            source: 'window.' + key + ' (object)',
                                            url: url,
                                            context: jsonStr.substring(0, 200)
                                        });
                                    }
                                });
                            }
                        }
                    } catch (e) {
                        // å¿½ç•¥è®¿é—®é”™è¯¯
                    }
                }
                
                return urls;
            }
        """)
        
        print(f"    æ‰¾åˆ° {len(global_urls)} ä¸ªå…¨å±€å˜é‡ä¸­çš„URL")
        for url_info in global_urls[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"      - {url_info['url']}")
            print(f"        æ¥æº: {url_info['source']}")
        
        # 7.2 æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„URL
        print("  7.2 æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„URL...")
        script_urls = await page.evaluate("""
            () => {
                const urls = [];
                const urlPattern = /https?:\/\/[^\s"'<>]+/g;
                const scripts = document.querySelectorAll('script');
                
                scripts.forEach((script, index) => {
                    if (script.textContent) {
                        const matches = script.textContent.match(urlPattern);
                        if (matches) {
                            matches.forEach(url => {
                                if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                    urls.push({
                                        source: 'script[' + index + ']',
                                        url: url,
                                        context: script.textContent.substring(0, 200)
                                    });
                                }
                            });
                        }
                    }
                });
                
                return urls;
            }
        """)
        
        print(f"    æ‰¾åˆ° {len(script_urls)} ä¸ªscriptæ ‡ç­¾ä¸­çš„URL")
        for url_info in script_urls[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"      - {url_info['url']}")
            print(f"        æ¥æº: {url_info['source']}")
        
        # 7.3 æŸ¥æ‰¾dataå±æ€§ä¸­çš„URL
        print("  7.3 æŸ¥æ‰¾dataå±æ€§ä¸­çš„URL...")
        data_urls = await page.evaluate("""
            () => {
                const urls = [];
                const urlPattern = /https?:\/\/[^\s"'<>]+/g;
                const elements = document.querySelectorAll('*');
                
                elements.forEach((element, index) => {
                    // æ£€æŸ¥æ‰€æœ‰data-*å±æ€§
                    for (let attr of element.attributes) {
                        if (attr.name.startsWith('data-')) {
                            const matches = attr.value.match(urlPattern);
                            if (matches) {
                                matches.forEach(url => {
                                    if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                        urls.push({
                                            source: element.tagName + '[' + index + '].' + attr.name,
                                            url: url,
                                            context: attr.value.substring(0, 200)
                                        });
                                    }
                                });
                            }
                        }
                    }
                });
                
                return urls;
            }
        """)
        
        print(f"    æ‰¾åˆ° {len(data_urls)} ä¸ªdataå±æ€§ä¸­çš„URL")
        for url_info in data_urls[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"      - {url_info['url']}")
            print(f"        æ¥æº: {url_info['source']}")
        
        # 7.4 æŸ¥æ‰¾LocalStorageå’ŒSessionStorage
        print("  7.4 æŸ¥æ‰¾å­˜å‚¨ä¸­çš„URL...")
        storage_urls = await page.evaluate("""
            () => {
                const urls = [];
                const urlPattern = /https?:\/\/[^\s"'<>]+/g;
                
                // æ£€æŸ¥localStorage
                for (let i = 0; i < localStorage.length; i++) {
                    const key = localStorage.key(i);
                    const value = localStorage.getItem(key);
                    const matches = value.match(urlPattern);
                    if (matches) {
                        matches.forEach(url => {
                            if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                urls.push({
                                    source: 'localStorage.' + key,
                                    url: url,
                                    context: value.substring(0, 200)
                                });
                            }
                        });
                    }
                }
                
                // æ£€æŸ¥sessionStorage
                for (let i = 0; i < sessionStorage.length; i++) {
                    const key = sessionStorage.key(i);
                    const value = sessionStorage.getItem(key);
                    const matches = value.match(urlPattern);
                    if (matches) {
                        matches.forEach(url => {
                            if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis')) {
                                urls.push({
                                    source: 'sessionStorage.' + key,
                                    url: url,
                                    context: value.substring(0, 200)
                                });
                            }
                        });
                    }
                }
                
                return urls;
            }
        """)
        
        print(f"    æ‰¾åˆ° {len(storage_urls)} ä¸ªå­˜å‚¨ä¸­çš„URL")
        for url_info in storage_urls[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"      - {url_info['url']}")
            print(f"        æ¥æº: {url_info['source']}")
        
        # 7.5 æŸ¥æ‰¾ç½‘ç»œè¯·æ±‚æ•°æ®
        print("  7.5 æŸ¥æ‰¾é¡µé¢HTMLä¸­çš„URL...")
        html_urls = await page.evaluate("""
            () => {
                const urls = [];
                const urlPattern = /https?:\/\/[^\s"'<>]+/g;
                const html = document.documentElement.outerHTML;
                const matches = html.match(urlPattern);
                
                if (matches) {
                    const uniqueUrls = [...new Set(matches)];
                    uniqueUrls.forEach(url => {
                        if (!url.includes('deepseek.com') && !url.includes('intercom') && !url.includes('googleapis') && !url.includes('.js') && !url.includes('.css')) {
                            urls.push({
                                source: 'HTML content',
                                url: url,
                                context: 'Found in page HTML'
                            });
                        }
                    });
                }
                
                return urls;
            }
        """)
        
        print(f"    æ‰¾åˆ° {len(html_urls)} ä¸ªHTMLä¸­çš„URL")
        for url_info in html_urls[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"      - {url_info['url']}")
        
        # 8. åˆå¹¶å’Œåˆ†æç»“æœ
        all_urls = global_urls + script_urls + data_urls + storage_urls + html_urls
        
        # å»é‡
        unique_urls = {}
        for url_info in all_urls:
            url = url_info['url']
            if url not in unique_urls:
                unique_urls[url] = url_info
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        sorted_urls = []
        for url, info in unique_urls.items():
            score = 0
            url_lower = url.lower()
            context_lower = info['context'].lower()
            
            # å°é¸¡ç§‘æŠ€ç›¸å…³
            if 'å°é¸¡' in context_lower or 'xiaoji' in url_lower or 'gamesir' in url_lower:
                score += 20
            
            # æ–°é—»å’Œæ–‡ç« ç½‘ç«™
            news_domains = ['36kr.com', 'zhihu.com', 'baidu.com', 'sohu.com', 'sina.com', 'qq.com', 'ithome.com']
            for domain in news_domains:
                if domain in url_lower:
                    score += 15
                    break
            
            # æ–‡ç« è·¯å¾„
            if '/article/' in url_lower or '/news/' in url_lower or '.html' in url_lower:
                score += 10
            
            info['score'] = score
            sorted_urls.append(info)
        
        sorted_urls.sort(key=lambda x: x['score'], reverse=True)
        
        # 9. ä¿å­˜ç»“æœ
        mining_result = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'total_urls_found': len(unique_urls),
            'global_urls': len(global_urls),
            'script_urls': len(script_urls),
            'data_urls': len(data_urls),
            'storage_urls': len(storage_urls),
            'html_urls': len(html_urls),
            'sorted_urls': sorted_urls[:20]  # ä¿å­˜å‰20ä¸ªæœ€ç›¸å…³çš„
        }
        
        result_filename = f"js_data_mining_{timestamp}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(mining_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ•°æ®æŒ–æ˜ç»“æœå·²ä¿å­˜: {result_filename}")
        print(f"\nğŸ¯ æŒ–æ˜æ€»ç»“:")
        print(f"  - æ€»å…±æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€URL")
        print(f"  - å…¨å±€å˜é‡: {len(global_urls)} ä¸ª")
        print(f"  - Scriptæ ‡ç­¾: {len(script_urls)} ä¸ª")
        print(f"  - Dataå±æ€§: {len(data_urls)} ä¸ª")
        print(f"  - å­˜å‚¨: {len(storage_urls)} ä¸ª")
        print(f"  - HTMLå†…å®¹: {len(html_urls)} ä¸ª")
        
        if sorted_urls:
            print(f"\nğŸ“„ æœ€ç›¸å…³çš„URL (æŒ‰å¾—åˆ†æ’åº):")
            for i, url_info in enumerate(sorted_urls[:8], 1):
                print(f"  {i}. {url_info['url']}")
                print(f"     å¾—åˆ†: {url_info['score']}")
                print(f"     æ¥æº: {url_info['source']}")
                print()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            if page and page.context:
                await page.context.close()
            if playwright:
                await playwright.stop()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
        except:
            pass


if __name__ == "__main__":
    asyncio.run(mine_javascript_data()) 