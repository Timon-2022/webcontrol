#!/usr/bin/env python3
"""
æŒä¹…ç™»å½•çŠ¶æ€çš„DeepSeekæºæå–å™¨
ä¸“é—¨è§£å†³ç™»å½•çŠ¶æ€ä¿æŒé—®é¢˜ï¼Œé¿å…æ¯æ¬¡éƒ½éœ€è¦é‡æ–°ç™»å½•
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from playwright.async_api import async_playwright
import re
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PersistentLoginExtractor:
    """æŒä¹…ç™»å½•çŠ¶æ€çš„æºæå–å™¨"""
    
    def __init__(self):
        self.browser = None
        self.page = None
        self.playwright = None
        self.login_state_file = "login_state.json"
        self.user_data_dir = "./deepseek_user_data"  # ç”¨æˆ·æ•°æ®ç›®å½•

    async def init_browser_with_persistent_login(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å¹¶ä¿æŒç™»å½•çŠ¶æ€"""
        try:
            self.playwright = await async_playwright().start()
            
            # ä½¿ç”¨æŒä¹…åŒ–çš„ç”¨æˆ·æ•°æ®ç›®å½•
            context = await self.playwright.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir,
                headless=False,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled'
                ]
            )
            
            # è·å–ç¬¬ä¸€ä¸ªé¡µé¢æˆ–åˆ›å»ºæ–°é¡µé¢
            if context.pages:
                self.page = context.pages[0]
            else:
                self.page = await context.new_page()
            
            logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æŒä¹…åŒ–ç”¨æˆ·æ•°æ®ï¼‰")
            return True
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def check_login_status(self):
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            # è®¿é—®DeepSeek
            await self.page.goto("https://chat.deepseek.com", timeout=30000)
            await self.page.wait_for_timeout(3000)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•ï¼ˆæŸ¥æ‰¾èŠå¤©è¾“å…¥æ¡†ï¼‰
            try:
                chat_input = await self.page.wait_for_selector("textarea", timeout=5000)
                if chat_input:
                    logger.info("âœ… å·²ç™»å½•çŠ¶æ€")
                    return True
            except:
                pass
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®
            login_selectors = [
                "text=ç™»å½•",
                "text=Login", 
                "[class*='login']",
                "button:has-text('ç™»å½•')",
                "a:has-text('ç™»å½•')"
            ]
            
            for selector in login_selectors:
                try:
                    login_element = await self.page.query_selector(selector)
                    if login_element:
                        logger.warning("âŒ éœ€è¦ç™»å½•")
                        return False
                except:
                    continue
            
            logger.info("âœ… å¯èƒ½å·²ç™»å½•")
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False

    async def manual_login_prompt(self):
        """æç¤ºæ‰‹åŠ¨ç™»å½•"""
        print("\n" + "="*80)
        print("ğŸ” éœ€è¦æ‰‹åŠ¨ç™»å½•")
        print("="*80)
        print("è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆç™»å½•ï¼š")
        print("1. ç‚¹å‡»ç™»å½•æŒ‰é’®")
        print("2. è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ")
        print("3. å®Œæˆä»»ä½•éªŒè¯æ­¥éª¤")
        print("4. ç¡®ä¿èƒ½çœ‹åˆ°èŠå¤©è¾“å…¥æ¡†")
        print("5. ç™»å½•å®Œæˆåï¼ŒæŒ‰å›è½¦é”®ç»§ç»­...")
        print("="*80)
        
        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        input("æŒ‰å›è½¦é”®ç»§ç»­ï¼ˆç¡®ä¿å·²å®Œæˆç™»å½•ï¼‰...")
        
        # å†æ¬¡æ£€æŸ¥ç™»å½•çŠ¶æ€
        if await self.check_login_status():
            logger.info("âœ… ç™»å½•ç¡®è®¤æˆåŠŸ")
            return True
        else:
            logger.error("âŒ ç™»å½•ç¡®è®¤å¤±è´¥")
            return False

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page and self.page.context:
                await self.page.context.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")

    def is_valuable_article_url(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰ä»·å€¼çš„æ–‡ç« é¡µé¢URL"""
        if not url or len(url) < 10:
            return False
        
        url_lower = url.lower()
        
        # è¿‡æ»¤é™æ€èµ„æº
        exclude_patterns = [
            '.css', '.js', '.png', '.jpg', '.svg', '.ico', '.woff', '.ttf',
            '/static/', '/assets/', '/cdn/', 'fonts.googleapis', 'widget.',
            'analytics', 'tracking', 'facebook.com', 'twitter.com',
            'deepseek.com'  # æ’é™¤DeepSeekè‡ªå·±çš„é“¾æ¥
        ]
        
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return False
        
        # æ£€æŸ¥æœ‰ä»·å€¼çš„æ–‡ç« URLç‰¹å¾
        valuable_indicators = [
            '/article/', '/news/', '/post/', '/story/', '/detail/', '/content/',
            '.html', '.htm', '/p/', '/articles/', '/posts/', '/blog/',
            '/tech/', '/business/', '/finance/', '/company/', '/startup/',
            '/xinwen/', '/zixun/', '/baodao/', '/gonggao/'
        ]
        
        for indicator in valuable_indicators:
            if indicator in url_lower:
                return True
        
        # æ£€æŸ¥URLç»“æ„
        parsed = urlparse(url)
        path = parsed.path
        
        if path and path.count('/') >= 2:
            if re.search(r'/\d+', path) or re.search(r'/\d{4}[/-]\d{2}', path):
                return True
            if len(path) > 10 and not path.endswith('/'):
                return True
        
        return False

    def calculate_article_score(self, url, text, title):
        """è®¡ç®—æ–‡ç« ç›¸å…³æ€§å¾—åˆ†"""
        score = 0.0
        url_lower = url.lower()
        text_lower = text.lower()
        title_lower = title.lower()
        
        # å°é¸¡ç§‘æŠ€ç›¸å…³å…³é”®è¯
        if 'å°é¸¡' in text_lower or 'xiaoji' in text_lower or 'å°é¸¡' in title_lower:
            score += 25.0
        if 'gamesir' in url_lower or 'gamesir' in text_lower or 'gamesir' in title_lower:
            score += 20.0
        
        # ä¸šåŠ¡å…³é”®è¯
        business_keywords = ['ç§‘æŠ€', 'å…¬å¸', 'ä¼ä¸š', 'æ¸¸æˆ', 'æ‰‹æŸ„', 'å¤–è®¾', 'ç¡¬ä»¶', 'æ§åˆ¶å™¨']
        for keyword in business_keywords:
            if keyword in text_lower or keyword in title_lower:
                score += 8.0
        
        # æƒå¨ç½‘ç«™
        authoritative_domains = [
            '36kr.com', 'zhihu.com', 'baidu.com', 'sohu.com', 'sina.com.cn',
            'qq.com', 'tencent.com', 'qcc.com', 'tianyancha.com', 'ithome.com'
        ]
        
        for domain in authoritative_domains:
            if domain in url_lower:
                score += 15.0
                break
        
        # URLç»“æ„è¯„åˆ†
        if '/article/' in url_lower or '/news/' in url_lower:
            score += 12.0
        if re.search(r'/\d{4}[/-]\d{2}', url_lower):
            score += 10.0
        if url_lower.endswith('.html'):
            score += 8.0
        
        return score

    async def wait_for_response_complete(self):
        """ç­‰å¾…å›å¤å®Œæˆ"""
        logger.info("ç­‰å¾…å›å¤å®Œæˆ...")
        start_time = time.time()
        stable_count = 0
        last_content = ""
        
        while time.time() - start_time < 45:  # å¢åŠ ç­‰å¾…æ—¶é—´
            try:
                elements = await self.page.query_selector_all(".ds-markdown.ds-markdown--block")
                if elements:
                    current_content = await elements[-1].inner_text()
                    if current_content == last_content:
                        stable_count += 1
                        if stable_count >= 4:
                            logger.info("å›å¤å†…å®¹å·²ç¨³å®š")
                            break
                    else:
                        stable_count = 0
                        last_content = current_content
                
                await self.page.wait_for_timeout(2000)
            except:
                break
        
        await self.page.wait_for_timeout(5000)

    async def find_sources_info(self):
        """æŸ¥æ‰¾æºä¿¡æ¯"""
        try:
            selectors = [
                "text=å·²æœç´¢åˆ°",
                "[class*='source']",
                "text=/å·²æœç´¢åˆ°\\d+ä¸ªç½‘é¡µ/",
                "text=/æœç´¢åˆ°\\d+/",
                "text=/\\d+ä¸ªç½‘é¡µ/"
            ]
            
            for selector in selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for element in elements:
                        text = await element.inner_text()
                        if 'æœç´¢åˆ°' in text and ('ç½‘é¡µ' in text or 'ä¸ª' in text):
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                count = int(numbers[0])
                                logger.info(f"æ‰¾åˆ°æºä¿¡æ¯: {text}")
                                return {'count': count, 'element': element, 'text': text}
                except:
                    continue
            
            return {'count': 0, 'element': None, 'text': ''}
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æºä¿¡æ¯å¤±è´¥: {e}")
            return {'count': 0, 'element': None, 'text': ''}

    async def extract_article_links(self):
        """æå–æ–‡ç« é“¾æ¥"""
        references = []
        
        try:
            logger.info("å¼€å§‹æå–æ–‡ç« é“¾æ¥...")
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await self.page.wait_for_timeout(8000)
            
            # å¤šæ¬¡æ»šåŠ¨åŠ è½½å†…å®¹
            for i in range(6):
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await self.page.wait_for_timeout(1500)
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            all_links = await self.page.query_selector_all("a[href]")
            logger.info(f"æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            # åˆ†æé“¾æ¥
            for i, link in enumerate(all_links):
                try:
                    href = await link.get_attribute('href')
                    if not href:
                        continue
                    
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if href.startswith('/'):
                        href = 'https://chat.deepseek.com' + href
                    elif not href.startswith('http'):
                        continue
                    
                    text = await link.inner_text()
                    title = await link.get_attribute('title') or ''
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰ä»·å€¼çš„æ–‡ç« URL
                    if self.is_valuable_article_url(href):
                        score = self.calculate_article_score(href, text, title)
                        
                        if score > 5.0:
                            references.append({
                                'url': href,
                                'text': text.strip()[:120],
                                'title': title.strip()[:120],
                                'score': score,
                                'domain': urlparse(href).netloc
                            })
                            
                            logger.info(f"å‘ç°æ–‡ç« é“¾æ¥: {href[:60]}... (å¾—åˆ†: {score:.1f})")
                    
                except Exception as e:
                    continue
            
            # æ’åºå’Œå»é‡
            references.sort(key=lambda x: x['score'], reverse=True)
            
            seen_urls = set()
            unique_refs = []
            for ref in references:
                if ref['url'] not in seen_urls:
                    seen_urls.add(ref['url'])
                    unique_refs.append(ref)
            
            logger.info(f"æå–åˆ° {len(unique_refs)} ä¸ªå”¯ä¸€æ–‡ç« é“¾æ¥")
            return unique_refs[:15]
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []

    async def run_extraction_with_persistent_login(self, query):
        """è¿è¡Œå¸¦æŒä¹…ç™»å½•çš„æå–æµç¨‹"""
        result = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'success': False,
            'login_required': False,
            'sources_count': 0,
            'article_references': [],
            'error': '',
            'steps': []
        }
        
        try:
            # 1. æ£€æŸ¥ç™»å½•çŠ¶æ€
            logger.info("1. æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self.check_login_status():
                result['login_required'] = True
                if not await self.manual_login_prompt():
                    result['error'] = "ç™»å½•å¤±è´¥"
                    return result
            result['steps'].append('ç¡®è®¤ç™»å½•çŠ¶æ€')
            
            # 2. å‘é€æŸ¥è¯¢
            logger.info("2. å‘é€æŸ¥è¯¢...")
            chat_input = await self.page.wait_for_selector("textarea", timeout=10000)
            await chat_input.fill(query)
            await chat_input.press('Enter')
            result['steps'].append('å‘é€æŸ¥è¯¢')
            
            # 3. ç­‰å¾…å›å¤å®Œæˆ
            logger.info("3. ç­‰å¾…å›å¤å®Œæˆ...")
            await self.wait_for_response_complete()
            result['steps'].append('ç­‰å¾…å›å¤å®Œæˆ')
            
            # 4. æŸ¥æ‰¾æºä¿¡æ¯
            logger.info("4. æŸ¥æ‰¾æºä¿¡æ¯...")
            sources_info = await self.find_sources_info()
            result['sources_count'] = sources_info['count']
            result['steps'].append('æŸ¥æ‰¾æºä¿¡æ¯')
            
            # 5. ç‚¹å‡»æºé“¾æ¥
            if sources_info['element']:
                logger.info("5. ç‚¹å‡»æºé“¾æ¥...")
                try:
                    await sources_info['element'].click()
                    await self.page.wait_for_timeout(5000)
                    
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    await self.page.screenshot(path=f"persistent_click_{timestamp}.png")
                    result['steps'].append('ç‚¹å‡»æºé“¾æ¥')
                except Exception as e:
                    logger.warning(f"ç‚¹å‡»æºé“¾æ¥å¤±è´¥: {e}")
            
            # 6. æå–æ–‡ç« é“¾æ¥
            logger.info("6. æå–æ–‡ç« é“¾æ¥...")
            article_refs = await self.extract_article_links()
            result['article_references'] = article_refs
            result['steps'].append('æå–æ–‡ç« é“¾æ¥')
            
            result['success'] = True
            logger.info(f"æå–å®Œæˆ: {result['sources_count']} ä¸ªæºï¼Œ{len(article_refs)} ä¸ªæ–‡ç« é“¾æ¥")
            
        except Exception as e:
            logger.error(f"æå–è¿‡ç¨‹å‡ºé”™: {e}")
            result['error'] = str(e)
        
        return result

    def save_and_print_result(self, result):
        """ä¿å­˜å¹¶æ‰“å°ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"persistent_result_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"ç»“æœå·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {e}")
        
        # æ‰“å°ç»“æœ
        print("\n" + "="*80)
        print("ğŸ” æŒä¹…ç™»å½•ç‰ˆDeepSeekæºæå–ç»“æœ")
        print("="*80)
        print(f"æŸ¥è¯¢: {result['query']}")
        print(f"æˆåŠŸ: {'âœ…' if result['success'] else 'âŒ'}")
        print(f"éœ€è¦ç™»å½•: {'æ˜¯' if result['login_required'] else 'å¦'}")
        print(f"æœç´¢ç½‘é¡µæ•°: {result['sources_count']}")
        print(f"æå–æ–‡ç« æ•°: {len(result['article_references'])}")
        print(f"å®Œæˆæ­¥éª¤: {' â†’ '.join(result['steps'])}")
        
        if result['article_references']:
            print(f"\nğŸ“„ æå–åˆ°çš„æ–‡ç« é“¾æ¥ (æŒ‰ç›¸å…³æ€§æ’åº):")
            print("-" * 80)
            for i, ref in enumerate(result['article_references'][:10], 1):
                print(f"{i}. {ref['text'][:50]}...")
                print(f"   ğŸ”— URL: {ref['url']}")
                print(f"   ğŸ¢ åŸŸå: {ref['domain']}")
                print(f"   â­ ç›¸å…³æ€§å¾—åˆ†: {ref['score']:.1f}")
                if ref['title']:
                    print(f"   ğŸ“ æ ‡é¢˜: {ref['title'][:50]}...")
                print()
        
        if result['error']:
            print(f"âŒ é”™è¯¯: {result['error']}")
        
        return filename


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æŒä¹…ç™»å½•ç‰ˆDeepSeekæºæå–å™¨")
    print("è‡ªåŠ¨ä¿æŒç™»å½•çŠ¶æ€ï¼Œé¿å…é‡å¤ç™»å½•")
    print("="*80)
    
    extractor = PersistentLoginExtractor()
    
    try:
        if not await extractor.init_browser_with_persistent_login():
            print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
            return
        
        query = "å°é¸¡ç§‘æŠ€çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¬å¸èƒŒæ™¯ã€ä¸šåŠ¡èŒƒå›´ã€æœ€æ–°åŠ¨æ€"
        print(f"ğŸ¯ æ‰§è¡ŒæŸ¥è¯¢: {query}")
        
        result = await extractor.run_extraction_with_persistent_login(query)
        filename = extractor.save_and_print_result(result)
        
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")
        print("\nğŸ’¡ æç¤ºï¼šä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨ä¿å­˜çš„ç™»å½•çŠ¶æ€")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await extractor.close_browser()


if __name__ == "__main__":
    asyncio.run(main())